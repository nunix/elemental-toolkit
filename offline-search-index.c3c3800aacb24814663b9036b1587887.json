[{"body":"Elemental-toolkit consists of a CLI program that is used to install a system and build bootable sources. The CLI also embeds configuration needed for a bootable derivative.\nDownload Elemental Elemental toolkit can be run directly using a container runtime such as docker:\ndocker run -it --rm ghcr.io/rancher/elemental-toolkit/elemental-cli:latest version Building from source The CLI can also be built from source by checking out the repo and running make:\ngit clone https://github.com/rancher/elemental-toolkit cd elemental-toolkit make build-cli ./build/elemental version What to do next? Check out the customization section to build a custom Elemental derivative or the example section for some already prepared recipe examples.\n","categories":"","description":"How to get Elemental Toolkit\n","excerpt":"How to get Elemental Toolkit\n","ref":"/docs/getting-started/download/","tags":"","title":"Download"},{"body":"When building a elemental-toolkit derivative, a common set of packages are required with a common default configuration. Some of the most notably are:\nsystemd as init system grub for boot loader dracut for initramfs ","categories":"","description":"Package stack for derivatives\n","excerpt":"Package stack for derivatives\n","ref":"/docs/creating-derivatives/package_stack/","tags":"","title":"Package stack"},{"body":"We have a custom augmented cloud-init syntax that allows to hook into various stages of the system, for example:\nInitramfs load Boot Network availability During upgrades, installation, deployments , and resets Cloud-init files in /system/oem, /oem and /usr/local/oem are applied in 5 different phases: boot, network, fs, initramfs and reconcile. All the available cloud-init keywords can be used in each stage. Additionally, it’s possible also to hook before or after a stage has run, each one has a specific stage which is possible to run steps: boot.after, network.before, fs.after etc.\nMultiple stages can be specified in a single cloud-init file.\nNote When a Elemental derivative boots it creates sentinel files in order to allow to execute cloud-init steps programmaticaly.\n/run/cos/recovery_mode is being created when booting from the recovery partition /run/cos/live_mode is created when booting from the LiveCD To execute a block using the sentinel files you can specify: if: '[ -f \"/run/cos/...\" ]', see the examples below.\nStages Below there is a detailed list of the stages available that can be used in the cloud-init configuration files\nrootfs This is the earliest stage, running before switching root, just right after the root is mounted in /sysroot and before applying the immutable rootfs configuration. This stage is executed over initrd root, no chroot is applied.\nExample:\nname: \"Set persistent devices\" stage: rootfs: - name: \"Layout configuration\" environment_file: /run/cos/cos-layout.env environment: VOLUMES: \"LABEL=COS_OEM:/oem LABEL=COS_PERSISTENT:/usr/local\" OVERLAY: \"tmpfs:25%\" initramfs This is still an early stage, running before switching root. Here you can apply radical changes to the booting setup of Elemental. Despite this is executed before switching root this exection runs chrooted into the target root after the immutable rootfs is set up and ready.\nExample:\nname: \"Run something on initramfs\" stages: initramfs: - name: \"Setting\" if: '[ ! -f \"/run/cos/recovery_mode\" ]' commands: - | # Run something when we are booting in active or passive touch /etc/something_important - name: \"Setting\" if: '[ -f \"/run/cos/recovery_mode\" ]' commands: - | # Run something when we are booting in recovery mode boot This stage is executed after initramfs has switched root, during the systemd bootup process.\nExample:\nname: \"Run something on boot\" stages: boot: - name: \"Setting\" if: '[ ! -f \"/run/cos/recovery_mode\" ]' commands: - | # Run something when we are booting in active or passive - name: \"Setting\" if: '[ -f \"/run/cos/recovery_mode\" ]' commands: - | # Run something when we are booting in recovery mode fs This stage is executed when fs is mounted and is guaranteed to have access to COS_STATE and COS_PERSISTENT.\nExample:\nname: \"Run something on boot\" stages: fs: - name: \"Setting\" if: '[ ! -f \"/run/cos/recovery_mode\" ]' commands: - | touch /usr/local/something - name: \"Setting\" if: '[ -f \"/run/cos/recovery_mode\" ]' commands: - | # Run something when we are booting in recovery mode network This stage is executed when network is available\nExample:\nname: \"Run something on boot\" stages: network: - name: \"Setting\" if: '[ ! -f \"/run/cos/recovery_mode\" ]' commands: - | # Network is available, do something.. reconcile This stage is executed 5m after boot and periodically each 60m.\nExample:\nname: \"Run something on boot\" stages: reconcile: - name: \"Setting\" if: '[ ! -f \"/run/sentinel\" ]' commands: - | touch /run/sentinel post-install This stage is executed after installation of the OS has ended (last step of elemental install).\nExample:\nname: \"Run something after installation\" stages: post-install: - name: \"Setting\" if: '[ ! -f \"/run/cos/recovery_mode\" ]' commands: - | # Run something when we are booting in active or passive - name: \"Setting\" if: '[ -f \"/run/cos/recovery_mode\" ]' commands: - | # Run something when we are booting in recovery mode after-install-chroot This stage is executed after installation of the OS filesystem image has completed. Note Steps executed at this stage are running inside the new OS as chroot, allowing to write persisting changes to the image, for example by installing additional software. Example:\nname: \"Run something after installation\" stages: after-install-chroot: - name: \"Setting\" commands: - | ... after-install This stage is executed after installation of the OS filesystem image has completed and just after the chroot hook. Note Steps executed at this stage are running when the new image and all the relevant partitions are still mounted in rw mode, allowing to write persisting changes to the image, for example installing additional software. Example:\nname: \"Run something after installation\" stages: after-install: - name: \"Setting\" commands: - | ... post-upgrade This stage is executed after upgrade of the OS has ended (last step of elemental upgrade).\nExample:\nname: \"Run something after upgrade\" stages: post-upgrade: - name: \"Setting\" if: '[ ! -f \"/run/cos/recovery_mode\" ]' commands: - | # Run something when we are booting in active or passive - name: \"Setting\" if: '[ -f \"/run/cos/recovery_mode\" ]' commands: - | # Run something when we are booting in recovery mode after-upgrade-chroot This stage is executed after installation of the OS filesystem image has completed. Note Steps executed at this stage are running inside the new OS as chroot, allowing to write persisting changes to the image, for example by downloading and installing additional software. Note Steps executed at this stage are based on stages found within the chroot, hence any new (not present in the current host) upgrade specific hook that requires to be executed during upgrade should be included here. Otherwise it will not be seen by elemental-cli during the upgrade. Example:\nname: \"Run something after upgrade\" stages: after-upgrade-chroot: - name: \"Setting\" commands: - | ... after-upgrade This stage is executed after installation of the OS filesystem image has completed and just after the chroot hook.\nExample:\nname: \"Run something after upgrade\" stages: after-upgrade: - name: \"Setting\" commands: - | ... post-reset This stage is executed after reset of the OS has ended (last step of elemental reset).\nExample:\nname: \"Run something after reset\" stages: post-reset: - name: \"Setting\" if: '[ ! -f \"/run/cos/recovery_mode\" ]' commands: - | # Run something when we are booting in active or passive - name: \"Setting\" if: '[ -f \"/run/cos/recovery_mode\" ]' commands: - | # Run something when we are booting in recovery mode after-reset-chroot This stage is executed after installation of the OS filesystem image has completed. Note Steps executed at this stage are running inside the new OS as chroot, allowing to write persisting changes to the image, for example by installing additional software. Example:\nname: \"Run something after installation\" stages: after-reset-chroot: - name: \"Setting\" commands: - | ... after-reset This stage is executed after installation of the OS filesystem image has completed and just after the chroot hook.\nExample:\nname: \"Run something after installation\" stages: after-reset: - name: \"Setting\" commands: - | ... before-install This stage is executed before installation (executed during elemental install).\nExample:\nname: \"Run something before installation\" stages: before-install: - name: \"Setting\" if: '[ ! -f \"/run/cos/recovery_mode\" ]' commands: - | # Run something when we are booting in active or passive - name: \"Setting\" if: '[ -f \"/run/cos/recovery_mode\" ]' commands: - | # Run something when we are booting in recovery mode before-upgrade This stage is executed before upgrade of the OS (executed during elemental upgrade).\nExample:\nname: \"Run something before upgrade\" stages: before-upgrade: - name: \"Setting\" if: '[ ! -f \"/run/cos/recovery_mode\" ]' commands: - | # Run something when we are booting in active or passive - name: \"Setting\" if: '[ -f \"/run/cos/recovery_mode\" ]' commands: - | # Run something when we are booting in recovery mode before-reset This stage is executed before reset of the OS (executed during elemental reset).\nExample:\nname: \"Run something before reset\" stages: before-reset: - name: \"Setting\" if: '[ ! -f \"/run/cos/recovery_mode\" ]' commands: - | # Run something when we are booting in active or passive - name: \"Setting\" if: '[ -f \"/run/cos/recovery_mode\" ]' commands: - | # Run something when we are booting in recovery mode ","categories":"","description":"Configure the system in the various stages: boot, initramfs, fs, network, reconcile\n","excerpt":"Configure the system in the various stages: boot, initramfs, fs, …","ref":"/docs/customizing/stages/","tags":"","title":"Stages"},{"body":"\nElemental toolkit provides a runtime and buildtime framework in order to boot containers in VMs, Baremetals and Cloud.\nYou can either choose to build a Elemental derivative or run Elemental to boostrap a new system.\nElemental vanilla images are published to allow to deploy user-built derivatives.\nElemental is designed to run, deploy and upgrade derivatives that can be built just as standard OCI container images. Elemental assets can be used to either drive unattended deployments of a derivative or used to create custom images (with packer).\nPhilosophy Philosophy behind elemental-toolkit is simple: it allows you to create Linux derivatives from container images.\nContainer registry as a single source of truth Hybrid way to access your image for different scopes (development, debugging, ..) No more inconsistent states between nodes. A “Store” to keep your (tagged) shared states where you can rollback and upgrade into. “Stateless”: Images with upgrades are rebuilt from scratch instead of applying upgrades. A/B upgrades, immutable systems The container image is booted as-is, encapsulating all the needed components (kernel, initrd included) and can be pulled locally for inspection, development and debugging. At the same time it can be used also to create installation medium as ISO, Raw images, OVA or Cloud specific images.\nA derivative automatically inherits the following featureset:\nCan upgrade to another container image Can deploy a system from scratch from an image Reset or recovery to a specific image Customize the image during runtime to persist changes across reboots Perform an installation from the LiveCD medium Building Elemental derivatives The starting point to use elemental-toolkit is to check out our examples and our creating bootable images section.\nThe only requirement to build derivatives with elemental-toolkit is Docker installed. If you are interested in building elemental-toolkit itself, see Development notes.\nThe toolkit itself is delivered as a set of standalone, re-usable OCI artifacts which are tagged and tracked as standard OCI images and it is installed inside the container image to provide the same featureset among derivatives, see how to create bootable images.\nVanilla images Elemental releases are composed of vanilla images that are used internally for testing and can be used as a starting point to deploy derivatives in specific environments (e.g. AWS) or just to try out the Elemental featureset.\nThe vanilla images ships no specific business-logic aside serving as a base for testing and deploying other derivatives.\nWhat to do next? Check out how to create bootable images or download the Elemental vanilla images to give Elemental a try!\nHere below you will find the common documentation that applies to any derivative built with Elemental and the Elemental vanilla images.\n","categories":"","description":"Getting started with Elemental\n","excerpt":"Getting started with Elemental\n","ref":"/docs/getting-started/","tags":"","title":"Getting Started"},{"body":"\nA derivative is a simple container image which can be processed by the Elemental toolkit in order to be bootable and installable. This section describes the requirements to create a container image that can be run by Elemental.\nRequirements Bootable images are standard container images, that means the usual build and push workflow applies, and building images is also a way to persist oem customizations.\nThe base image can be any Linux distribution that is compatible with our flavors.\nThe image needs to ship:\nparts of the elemental-toolkit (required, see below) kernel (required) initrd (required) grub2 (required) dracut (required) microcode (optional, not required in order to boot, but recomended) cosign packages (optional, required if you want to verify the images) Example An illustrative example can be:\n# run `make build` to build local/elemental-toolkit image ARG TOOLKIT_REPO ARG VERSION FROM ${TOOLKIT_REPO}:${VERSION} as TOOLKIT # OS base image of our choice FROM opensuse/leap:15.4 as OS ARG REPO ARG VERSION ENV VERSION=${VERSION} # install kernel, systemd, dracut, grub2 and other required tools RUN ARCH=$(uname -m); \\ if [[ $ARCH == \"aarch64\" ]]; then ARCH=\"arm64\"; fi; \\ zypper --non-interactive install --no-recommends -- \\ kernel-default \\ device-mapper \\ dracut \\ grub2 \\ grub2-${ARCH}-efi \\ shim \\ haveged \\ systemd \\ NetworkManager \\ openssh-server \\ openssh-clients \\ timezone \\ parted \\ e2fsprogs \\ dosfstools \\ mtools \\ xorriso \\ findutils \\ gptfdisk \\ rsync \\ squashfs \\ lvm2 \\ tar \\ gzip \\ vim \\ which \\ less \\ sudo \\ sed # Just add the elemental cli COPY --from=TOOLKIT /usr/bin/elemental /usr/bin/elemental # Enable essential services RUN systemctl enable NetworkManager.service # Enable /tmp to be on tmpfs RUN cp /usr/share/systemd/tmp.mount /etc/systemd/system # Generate initrd with required elemental services RUN elemental init -f \u0026\u0026 \\ kernel=$(ls /boot/Image-* | head -n1) \u0026\u0026 \\ if [ -e \"$kernel\" ]; then ln -sf \"${kernel#/boot/}\" /boot/vmlinuz; fi \u0026\u0026 \\ rm -rf /var/log/update* \u0026\u0026 \\ \u003e/var/log/lastlog \u0026\u0026 \\ rm -rf /boot/vmlinux* # Update os-release file with some metadata RUN echo IMAGE_REPO=\\\"${REPO}\\\" \u003e\u003e /etc/os-release \u0026\u0026 \\ echo IMAGE_TAG=\\\"${VERSION}\\\" \u003e\u003e /etc/os-release \u0026\u0026 \\ echo IMAGE=\\\"${REPO}:${VERSION}\\\" \u003e\u003e /etc/os-release \u0026\u0026 \\ echo TIMESTAMP=\"`date +'%Y%m%d%H%M%S'`\" \u003e\u003e /etc/os-release \u0026\u0026 \\ echo GRUB_ENTRY_NAME=\\\"Elemental\\\" \u003e\u003e /etc/os-release # Good for validation after the build CMD /bin/bash Complete source code: https://github.com/rancher/elemental-toolkit/blob/main/examples/green/Dockerfile In the example above, the elemental-toolkit parts that are required are pulled in by COPY --from=TOOLKIT /install-root /.\nInitrd The image should provide at least grub, systemd, dracut, a kernel and an initrd. Those are the common set of packages between derivatives. See also package stack. By default the initrd is expected to be symlinked to /boot/initrd and the kernel to /boot/vmlinuz, otherwise you can specify a custom path while building an iso and by customizing grub.\nBuilding The workflow would be then:\ndocker build the image docker push the image to some registry elemental upgrade --docker-image $IMAGE from a Elemental machine or (elemental reset if bootstrapping a cloud image) The following can be incorporated in any standard gitops workflow.\nYou can explore more examples in the example section on how to create bootable images.\nWhat’s next? Now that we have created our derivative container, we can either:\nBuild an iso ","categories":"","description":"This document describes the requirements to create standard container images that can be used for `Elemental` deployments\n","excerpt":"This document describes the requirements to create standard container …","ref":"/docs/creating-derivatives/creating_bootable_images/","tags":"","title":"Creating bootable images"},{"body":"Elemental (or any Elemental derivative built with elemental-toolkit) can be installed with elemental install:\nelemental install [options] \u003cdevice\u003e Option Description –cloud-init string Cloud-init config file –cosign Enable cosign verification (requires images with signatures) –cosign-key string Sets the URL of the public key to be used by cosign validation –eject-cd Try to eject the cd on reboot, only valid if booting from iso –firmware string Firmware to install for (’esp’ or ‘bios’) (default “efi”) –force Force install –help help for install –iso string Performs an installation from the ISO url –no-format Don’t format disks. It is implied that COS_STATE, COS_RECOVERY, COS_PERSISTENT, COS_OEM are already existing –verify Enable mtree checksum verification (requires images manifests generated with mtree separately) –part-table string Partition table type to use (default “gpt”) –poweroff Shutdown the system after install –reboot Reboot the system after install –recovery-system.uri string Sets the recovery image source and its type (e.g. ‘docker:registry.org/image:tag’) –system.uri string Sets the system image source and its type (e.g. ‘docker:registry.org/image:tag’) –strict Enable strict check of hooks (They need to exit with 0) –tty string Add named tty to grub Custom OEM configuration During installation it can be specified a cloud-init config file, that will be installed and persist in the system after installation:\nelemental install --cloud-init [url|path] \u003cdevice\u003e Custom partitioning When installing it’s possible to specify a custom partition sizes via the configuration file (/etc/elemental/config.yaml by default).\ninstall partitions: state: # All sizes are in MiB size: 8192 recovery: size: 4096 oem: size: 64 persistent: # zero size tells parted to use all the available # disk, note this is only functional for the last partition size: 0 Refer to the config file docs for further details about all partitioning options.\nIn order to create additional partitions please consider the layout section on cloud-init config file reference\nInstallation from 3rd party LiveCD or rescue mediums The installer can be used to perform installations also from outside the Elemental or standard derivative ISOs.\nFor instance, it is possible to install Elemental (or any derivative) with the installer from another bootable medium, or a rescue mode which is booting from RAM, given there is enough free RAM available.\nWith Docker If in the rescue system, or LiveCD you have docker available, it can be used to perform an installation\ndocker run --privileged -v /dev/:/dev/ -ti ghcr.io/rancher/elemental-toolkit/elemental-cli:latest install --system.uri $IMAGE $DEVICE Where $IMAGE is the container image that we want to install (e.g. oci:ghcr.io/rancher/elemental-toolkit/elemental-green:v0.10.7 ), elemental identifies the type of source by the URI scheme (docker, channel, dir or file). $DEVICE is the device where to perform the installation to (e.g. /dev/sda).\nNote, we used the ghcr.io/rancher/elemental-toolkit/elemental-cli:latest image which contains the latest stable installer and the dependencies. You can see all the versions at GitHub Container Registry.\nBy using manually the Elemental installer Similarly, the same mechanism can be used without docker. Install elemental using the Download guide and run the follow as root:\nelemental install --system.uri $IMAGE $DEVICE ","categories":"","description":"Installing Elemental or a derivative locally\n","excerpt":"Installing Elemental or a derivative locally\n","ref":"/docs/getting-started/install/","tags":"","title":"Installing"},{"body":"By default Elemental and derivatives are reading and executing cloud-init files in (lexicopgrahic) sequence inside:\n/system/oem /usr/local/cloud-config /oem It is also possible to run cloud-init file in a different location (URLs included, too) from boot cmdline by using the cos.setup=.. option.\nNote It is possible to install a custom cloud-init style file during install with --cloud-init flag on elemental install command or, it’s possible to add one or more files manually inside the /oem directory after installation. While /system/oem is reserved for system configurations to be included directly in the derivative container image, the /oem folder instead is reserved for persistent cloud-init files that can be extended in runtime.\nFor example, if you want to change /etc/issue of the system persistently, you can create /usr/local/cloud-config/90_after_install.yaml or alternatively in /oem/90_after_install.yaml with the following content:\n# The following is executed before fs is setted up: stages: fs: - name: \"After install\" files: - path: /etc/issue content: | Welcome, have fun! permissions: 0644 owner: 0 group: 0 - name: \"After install (second step)\" files: - path: /etc/motd content: | Welcome, have more fun! permissions: 0644 owner: 0 group: 0 For more examples you can find /system/oem inside Elemental vanilla images containing files used to configure on boot a pristine Elemental.\n","categories":"","description":"Persisting configurations in Elemental and derivatives\n","excerpt":"Persisting configurations in Elemental and derivatives\n","ref":"/docs/customizing/configuration_persistency/","tags":"","title":"Configuration persistency"},{"body":"Below is a reference of all keys available in the cloud-init style files.\nstages: # \"network\" is the stage where network is expected to be up # It is called internally when network is available from # the cos-setup-network unit. network: # Here there are a list of # steps to be run in the network stage - name: \"Some setup happening\" files: - path: /tmp/foo content: | test permissions: 0777 owner: 1000 group: 100 commands: - echo \"test\" modules: - nvidia environment: FOO: \"bar\" systctl: debug.exception-trace: \"0\" hostname: \"foo\" systemctl: enable: - foo disable: - bar start: - baz mask: - foobar authorized_keys: user: - \"github:suse\" - \"ssh-rsa ....\" dns: path: /etc/resolv.conf nameservers: - 8.8.8.8 ensure_entities: - path: /etc/passwd entity: | kind: \"user\" username: \"foo\" password: \"pass\" uid: 0 gid: 0 info: \"Foo!\" homedir: \"/home/foo\" shell: \"/bin/bash\" delete_entities: - path: /etc/passwd entity: | kind: \"user\" username: \"foo\" password: \"pass\" uid: 0 gid: 0 info: \"Foo!\" homedir: \"/home/foo\" shell: \"/bin/bash\" datasource: providers: - \"aws\" - \"digitalocean\" path: \"/etc/cloud-data\" The default cloud-config format is split into stages (initramfs, boot, network, initramfs, reconcile, called generically STAGE_ID below) see also stages that are emitted internally during the various phases by calling cos-setup STAGE_ID. steps (STEP_NAME below) defined for each stage are executed in order.\nEach cloud-config file is loaded and executed only at the apprioriate stage, this allows further components to emit their own stages at the desired time.\nThe cloud-init tool can be also run standalone, this helps debugging locally and also during development, you can find separate releases here.\nNote: Each cloud-init option can be either run in dot notation ( e.g. stages.network[0].authorized_keys.user=github:user ) in the boot args or either can supply a cloud-init URL at boot with the cos.setup=$URL parameter.\nUsing templates With Cloud Init support, templates can be used to allow dynamic configuration. More information about templates can be found here and also here for sprig functions.\nCompatibility with Cloud Init format A subset of the official cloud-config spec is implemented.\nIf a yaml file starts with #cloud-config it is parsed as a standard cloud-init and automatically associated it to the boot stage. For example:\n#cloud-config growpart: mode: auto devices: ['/'] users: - name: \"bar\" passwd: \"foo\" lock_passwd: true uid: \"1002\" groups: \"users\" ssh_authorized_keys: - faaapploo ssh_authorized_keys: - asdd runcmd: - foo hostname: \"bar\" write_files: - encoding: b64 content: CiMgVGhpcyBmaWxlIGNvbnRyb2xzIHRoZSBzdGF0ZSBvZiBTRUxpbnV4 path: /foo/bar permissions: \"0644\" owner: \"bar\" Is executed at boot, by using the standard cloud-config format.\nNote You can’t mix extended syntax with legacy cloud-init syntax. By pre-pending the cloud-config with the #cloud-config header you enable the legacy notation, and the extended one ( stages.. ) will be ignored. stages.STAGE_ID.STEP_NAME.name A description of the stage step. Used only when printing output to console.\nstages.STAGE_ID.STEP_NAME.files A list of files to write to disk.\nstages: default: - files: - path: /tmp/bar content: | #!/bin/sh echo \"test\" permissions: 0777 owner: 1000 group: 100 stages.STAGE_ID.STEP_NAME.directories A list of directories to be created on disk. Runs before files.\nstages: default: - name: \"Setup folders\" directories: - path: \"/etc/foo\" permissions: 0600 owner: 0 group: 0 stages.STAGE_ID.STEP_NAME.dns A way to configure the /etc/resolv.conf file.\nstages: default: - name: \"Setup dns\" dns: nameservers: - 8.8.8.8 - 1.1.1.1 search: - foo.bar options: - .. path: \"/etc/resolv.conf.bak\" stages.STAGE_ID.STEP_NAME.hostname A string representing the machine hostname. It sets it in the running system, updates /etc/hostname and adds the new hostname to /etc/hosts. Templates can be used to allow dynamic configuration. For example in mass-install scenario it could be needed (and easier) to specify hostnames for multiple machines from a single cloud-init config file.\nstages: default: - name: \"Setup hostname\" hostname: \"node-{{ trunc 4 .MachineID }}\" stages.STAGE_ID.STEP_NAME.sysctl Kernel configuration. It sets /proc/sys/\u003ckey\u003e accordingly, similarly to sysctl.\nstages: default: - name: \"Setup exception trace\" systctl: debug.exception-trace: \"0\" stages.STAGE_ID.STEP_NAME.authorized_keys A list of SSH authorized keys that should be added for each user. SSH keys can be obtained from GitHub user accounts by using the format github:${USERNAME}, similarly for Gitlab with gitlab:${USERNAME}.\nstages: default: - name: \"Setup exception trace\" authorized_keys: joe: - github:joe - ssh-rsa: ... stages.STAGE_ID.STEP_NAME.node If defined, the node hostname where this stage has to run, otherwise it skips the execution. The node can be also a regexp in the Golang format.\nstages: default: - name: \"Setup logging\" node: \"bastion\" stages.STAGE_ID.STEP_NAME.users A map of users and user info to set. Passwords can be also encrypted.\nThe users parameter adds or modifies the specified list of users. Each user is an object which consists of the following fields. Each field is optional and of type string unless otherwise noted. In case the user is already existing, the entry is ignored.\nname: Required. Login name of user gecos: GECOS comment of user passwd: Hash of the password to use for this user. Unencrypted strings are supported too. homedir: User’s home directory. Defaults to /home/name no-create-home: Boolean. Skip home directory creation. primary-group: Default group for the user. Defaults to a new group created named after the user. groups: Add user to these additional groups no-user-group: Boolean. Skip default group creation. ssh-authorized-keys: List of public SSH keys to authorize for this user system: Create the user as a system user. No home directory will be created. no-log-init: Boolean. Skip initialization of lastlog and faillog databases. shell: User’s login shell. stages: default: - name: \"Setup users\" users: bastion: passwd: \"strongpassword\" homedir: \"/home/foo stages.STAGE_ID.STEP_NAME.ensure_entities A user or a group in the entity format to be configured in the system\nstages: default: - name: \"Setup users\" ensure_entities: - path: /etc/passwd entity: | kind: \"user\" username: \"foo\" password: \"x\" uid: 0 gid: 0 info: \"Foo!\" homedir: \"/home/foo\" shell: \"/bin/bash\" stages.STAGE_ID.STEP_NAME.delete_entities A user or a group in the entity format to be pruned from the system\nstages: default: - name: \"Setup users\" delete_entities: - path: /etc/passwd entity: | kind: \"user\" username: \"foo\" password: \"x\" uid: 0 gid: 0 info: \"Foo!\" homedir: \"/home/foo\" shell: \"/bin/bash\" stages.STAGE_ID.STEP_NAME.modules A list of kernel modules to load.\nstages: default: - name: \"Setup users\" modules: - nvidia stages.STAGE_ID.STEP_NAME.systemctl A list of systemd services to enable, disable, mask or start.\nstages: default: - name: \"Setup users\" systemctl: enable: - systemd-timesyncd - cronie mask: - purge-kernels disable: - crond start: - cronie stages.STAGE_ID.STEP_NAME.environment A map of variables to write in /etc/environment, or otherwise specified in environment_file\nstages: default: - name: \"Setup users\" environment: FOO: \"bar\" stages.STAGE_ID.STEP_NAME.environment_file A string to specify where to set the environment file\nstages: default: - name: \"Setup users\" environment_file: \"/home/user/.envrc\" environment: FOO: \"bar\" stages.STAGE_ID.STEP_NAME.timesyncd Sets the systemd-timesyncd daemon file (/etc/system/timesyncd.conf) file accordingly. The documentation for timesyncd and all the options can be found here.\nstages: default: - name: \"Setup NTP\" systemctl: enable: - systemd-timesyncd timesyncd: NTP: \"0.pool.org foo.pool.org\" FallbackNTP: \"\" ... stages.STAGE_ID.STEP_NAME.commands A list of arbitrary commands to run after file writes and directory creation.\nstages: default: - name: \"Setup something\" commands: - echo 1 \u003e /bar stages.STAGE_ID.STEP_NAME.datasource Sets to fetch user data from the specified cloud providers. It populates provider specific data into /run/config folder and the custom user data is stored into the provided path.\nstages: default: - name: \"Fetch cloud provider's user data\" datasource: providers: - \"aws\" - \"digitalocean\" path: \"/etc/cloud-data\" stages.STAGE_ID.STEP_NAME.layout Sets additional partitions on disk free space, if any, and/or expands the last partition. All sizes are expressed in MiB only and default value of size: 0 means all available free space in disk. This plugin is useful to be used in oem images where the default partitions might not suit the actual disk geometry.\nstages: default: - name: \"Repart disk\" layout: device: # It will partition a device including the given filesystem label # or partition label (filesystem label matches first) or the device # provided in 'path'. The label check has precedence over path when # both are provided. label: \"COS_RECOVERY\" path: \"/dev/sda\" # Only last partition can be expanded and it happens before any other # partition is added. size: 0 means all available free space expand_partition: size: 4096 add_partitions: - fsLabel: \"COS_STATE\" size: 8192 # No partition label is applied if omitted pLabel: \"state\" - fsLabel: \"COS_PERSISTENT\" # default filesystem is ext2 if omitted filesystem: \"ext4\" ","categories":"","description":"Features inherited by Elemental derivatives that are also available in the Elemental vanilla images\n","excerpt":"Features inherited by Elemental derivatives that are also available in …","ref":"/docs/reference/cloud_init/","tags":"","title":"Cloud-init support"},{"body":"Elemental-toolkit provides some default configuration files for the following components:\nGRUB2 Dracut Cloud init files Boot assessment These configuration files can be installed into a Derivative using the elemental init-command\nThe init-command should be used inside the Dockerfile as in the following example:\n# run `make build` to build local/elemental-toolkit image ARG TOOLKIT_REPO ARG VERSION FROM ${TOOLKIT_REPO}:${VERSION} as TOOLKIT # OS base image of our choice FROM opensuse/leap:15.4 as OS ARG REPO ARG VERSION ENV VERSION=${VERSION} # install kernel, systemd, dracut, grub2 and other required tools RUN ARCH=$(uname -m); \\ if [[ $ARCH == \"aarch64\" ]]; then ARCH=\"arm64\"; fi; \\ zypper --non-interactive install --no-recommends -- \\ kernel-default \\ device-mapper \\ dracut \\ grub2 \\ grub2-${ARCH}-efi \\ shim \\ haveged \\ systemd \\ NetworkManager \\ openssh-server \\ openssh-clients \\ timezone \\ parted \\ e2fsprogs \\ dosfstools \\ mtools \\ xorriso \\ findutils \\ gptfdisk \\ rsync \\ squashfs \\ lvm2 \\ tar \\ gzip \\ vim \\ which \\ less \\ sudo \\ sed # Just add the elemental cli COPY --from=TOOLKIT /usr/bin/elemental /usr/bin/elemental # Enable essential services RUN systemctl enable NetworkManager.service # Enable /tmp to be on tmpfs RUN cp /usr/share/systemd/tmp.mount /etc/systemd/system # Generate initrd with required elemental services RUN elemental init -f \u0026\u0026 \\ kernel=$(ls /boot/Image-* | head -n1) \u0026\u0026 \\ if [ -e \"$kernel\" ]; then ln -sf \"${kernel#/boot/}\" /boot/vmlinuz; fi \u0026\u0026 \\ rm -rf /var/log/update* \u0026\u0026 \\ \u003e/var/log/lastlog \u0026\u0026 \\ rm -rf /boot/vmlinux* # Update os-release file with some metadata RUN echo IMAGE_REPO=\\\"${REPO}\\\" \u003e\u003e /etc/os-release \u0026\u0026 \\ echo IMAGE_TAG=\\\"${VERSION}\\\" \u003e\u003e /etc/os-release \u0026\u0026 \\ echo IMAGE=\\\"${REPO}:${VERSION}\\\" \u003e\u003e /etc/os-release \u0026\u0026 \\ echo TIMESTAMP=\"`date +'%Y%m%d%H%M%S'`\" \u003e\u003e /etc/os-release \u0026\u0026 \\ echo GRUB_ENTRY_NAME=\\\"Elemental\\\" \u003e\u003e /etc/os-release # Good for validation after the build CMD /bin/bash Complete source code: https://github.com/rancher/elemental-toolkit/blob/main/examples/green/Dockerfile The current features available for the init-command is:\nimmutable-rootfs: dracut configuration for mounting the immutable root filesystem. grub-config: grub configuration for booting the derivative. elemental-setup: services used for booting the system and running cloud-init files at boot/install/upgrade. dracut-config: default dracut configuration for generating an initrd. cloud-config-defaults: optional default settings for a derivative. cloud-config-essentials: essential cloud-init files. ","categories":"","description":"Extracting default system configuration\n","excerpt":"Extracting default system configuration\n","ref":"/docs/customizing/embedded_features/","tags":"","title":"Embedded configuration"},{"body":"By default you can login with the user root and cos in a vanilla Elemental image, this is also set automatically by the cloud-config-defaults feature if used by a derivative.\nYou can change this by overriding /system/oem/04_accounting.yaml in the container image if present, or via cloud-init.\nExamples Example accounting file ","categories":"","description":"Default login, and how to override it\n","excerpt":"Default login, and how to override it\n","ref":"/docs/customizing/login/","tags":"","title":"Login"},{"body":"There are several way to customize Elemental and a elemental-toolkit derivative:\ndeclaratively in runtime with cloud-config file (by overriding, or extending) stateful, embedding any configuration in the container image to be booted. For runtime persistence configuration, the only supported way is with cloud-config files, see the relevant docs.\nA derivative automatically loads and executes cloud-config files during the various system stages also inside /system/oem which is read-only and reserved to the system.\nDerivatives that wish to override default configurations can do that by placing extra cloud-init file, or overriding completely /system/oem in the target image.\nThis is to setup for example, the default root password or the preferred upgrade channel.\nThe following are the Elemental default oem files, which are shipped within the cloud-config-defaults and cloud-config-essentials features:\n/system/oem/00_rootfs.yaml - defines the rootfs mountpoint layout setting /system/oem/01_defaults.yaml - systemd defaults (keyboard layout, timezone) /system/oem/02_upgrades.yaml - Settings for Elemental vanilla channel upgrades /system/oem/03_branding.yaml - Branding setting, Derivative name, /etc/issue content /system/oem/04_accounting.yaml - Default user/pass /system/oem/05_network.yaml - Default network setup /system/oem/06_recovery.yaml - Executes additional commands when booting in recovery mode You can either override the above files, or alternatively not consume the above features while building a derivative.\n","categories":"","description":"OEM configuration reserved to Elemental and derivatives\n","excerpt":"OEM configuration reserved to Elemental and derivatives\n","ref":"/docs/customizing/oem_configuration/","tags":"","title":"OEM configuration"},{"body":"Elemental and derivatives are immutable systems. That means that any change in the running OS will not persist after a reboot.\nWhile configurations can be persisted, there are occasions where installing a custom package or provide additional persistent files in the end system is needed.\nWe will see here a way to install packages, drivers, or apply any modification we might want to do in the OS image during runtime, without any need to rebuild the derivative container image. This will let any user (and not derivative developer) to apply any needed customization and to be able to persist across upgrades.\nTransient changes To apply transient changes, it’s possible to boot a Elemental derivative in read/write mode by specifying rd.cos.debugrw see here for more details. This allows to do any change and will persist into the active/passive booting system (does NOT apply for recovery). Altough this methodology should be only considered for debugging purposes.\nPersist changes with Cloud init files Elemental allows to apply a set of commands, or cloud-init steps, during upgrade, deploy, install and reset in the context of the target image, in RW capabilities. This allows to carry on changes during upgrades on the target image without the need to re-build or have a custom derivative image.\nAll the configuration that we want to apply to the system will run each time we do an upgrade, a reset or an installation on top of the new downloaded image (in case of upgrade) or the image which is the target system.\nBetween the available stages in the cloud-init there are after-upgrade-chroot, after-install-chroot, after-reset-chroot and after-deploy-chroot, for example, consider the following cloud-init file:\nstages: name: \"Install something\" stages: after-upgrade-chroot: - commands: - zypper in -y ... after-reset-chroot: - commands: - zypper in -y ... after-deploy-chroot: - commands: - zypper in -y ... after-install-chroot: - commands: - zypper in -y ... It will run the zypper in -y ... calls during each stage, in the context of the target system, allowing to customize the target image with additional packages.\nNote zypper calls here are just an example. We could have used dnf for fedora based, or apt-get for ubuntu based images. When running the cloud-init steps the /oem partition and /usr/local will be mounted to COS_OEM and COS_PERSISTENT respectively, allowing to load extra data (e.g. rpm files, or configuration).\nExample If an user wants to install an additional package in the running system, and keep having that persistent across upgrades, he can copy the following file (install.yaml) inside the /oem folder, or /usr/local/cloud-config:\nstages: name: \"Install something\" stages: after-upgrade-chroot: - commands: - zypper in -y vim and run elemental upgrade.\nIt will automatically upgrade the system with the changes above included.\n","categories":"","description":"Applying changes to Elemental images in runtime or “how to install a package in an immutable OS at runtime?”\n","excerpt":"Applying changes to Elemental images in runtime or “how to install a …","ref":"/docs/customizing/runtime_persistent_changes/","tags":"","title":"Runtime persistent changes"},{"body":"You can find the examples below in the examples folder.\nFrom standard images Besides using the elemental-toolkit toolchain, it’s possible to create standard container images which are consumable by the vanilla Elemental images (ISO, Cloud Images, etc.) during the upgrade and deploy phase.\nAn example of a Dockerfile image can be:\n# run `make build` to build local/elemental-toolkit image ARG TOOLKIT_REPO ARG VERSION FROM ${TOOLKIT_REPO}:${VERSION} as TOOLKIT # OS base image of our choice FROM opensuse/leap:15.4 as OS ARG REPO ARG VERSION ENV VERSION=${VERSION} # install kernel, systemd, dracut, grub2 and other required tools RUN ARCH=$(uname -m); \\ if [[ $ARCH == \"aarch64\" ]]; then ARCH=\"arm64\"; fi; \\ zypper --non-interactive install --no-recommends -- \\ kernel-default \\ device-mapper \\ dracut \\ grub2 \\ grub2-${ARCH}-efi \\ shim \\ haveged \\ systemd \\ NetworkManager \\ openssh-server \\ openssh-clients \\ timezone \\ parted \\ e2fsprogs \\ dosfstools \\ mtools \\ xorriso \\ findutils \\ gptfdisk \\ rsync \\ squashfs \\ lvm2 \\ tar \\ gzip \\ vim \\ which \\ less \\ sudo \\ sed # Just add the elemental cli COPY --from=TOOLKIT /usr/bin/elemental /usr/bin/elemental # Enable essential services RUN systemctl enable NetworkManager.service # Enable /tmp to be on tmpfs RUN cp /usr/share/systemd/tmp.mount /etc/systemd/system # Generate initrd with required elemental services RUN elemental init -f \u0026\u0026 \\ kernel=$(ls /boot/Image-* | head -n1) \u0026\u0026 \\ if [ -e \"$kernel\" ]; then ln -sf \"${kernel#/boot/}\" /boot/vmlinuz; fi \u0026\u0026 \\ rm -rf /var/log/update* \u0026\u0026 \\ \u003e/var/log/lastlog \u0026\u0026 \\ rm -rf /boot/vmlinux* # Update os-release file with some metadata RUN echo IMAGE_REPO=\\\"${REPO}\\\" \u003e\u003e /etc/os-release \u0026\u0026 \\ echo IMAGE_TAG=\\\"${VERSION}\\\" \u003e\u003e /etc/os-release \u0026\u0026 \\ echo IMAGE=\\\"${REPO}:${VERSION}\\\" \u003e\u003e /etc/os-release \u0026\u0026 \\ echo TIMESTAMP=\"`date +'%Y%m%d%H%M%S'`\" \u003e\u003e /etc/os-release \u0026\u0026 \\ echo GRUB_ENTRY_NAME=\\\"Elemental\\\" \u003e\u003e /etc/os-release # Good for validation after the build CMD /bin/bash Complete source code: https://github.com/rancher/elemental-toolkit/blob/main/examples/green/Dockerfile We can just run docker to build the image with\ndocker build -t $IMAGE . The important piece is that an image needs to ship at least:\ngrub2 systemd kernel dracut And then extract the configuration for the system using the elemental init-command.\nCustomizations All the method above imply that the image generated will be the booting one, there are however several configuration entrypoint that you should keep in mind while building the image:\nEverything under /system/oem will be loaded during the various stage (boot, network, initramfs). You can check here for the Elemental defaults. See 00_rootfs.yaml to customize the booting layout. /etc/cos/bootargs.cfg contains the booting options required to boot the image with GRUB ","categories":"","description":"This document describes the requirements to create standard container images that can be used for `Elemental` deployments\n","excerpt":"This document describes the requirements to create standard container …","ref":"/docs/examples/creating_bootable_images/","tags":"","title":"Creating bootable images"},{"body":"Elemental during installation, reset and upgrade (elemental install, elemental reset and elemental upgrade respectively) will read a configuration file in order to apply derivative customizations. The configuration files are sourced in precedence order and can be located in the following places:\n/etc/os-release \u003cconfig-dir\u003e/config.yaml \u003cconfig-dir\u003e/config.d/*.yaml By default \u003cconfig-dir\u003e is set to /etc/elemental however this can be changed to any custom path by using the --config-dir runtime flag.\nBelow you can find an example of the config file including most of the available options:\n# DISCLAIMER: this document show most of all available options. Elemental # uses defaults if any of this value is missing. Values shown here # only represent an example and they should not be used as defaults. # installation configuration for 'install' command install: # target is the only value that has no default, it must be provided by # config, flags or env variables. target: /dev/sda # basic disk configs for partitioning ('efi|bios' and 'gpt|msdos') firmware: efi part-table: gpt # partitions setup # setting a partition size key to 0 means that the partition will take over the rest of the free space on the disk # after creating the rest of the partitions # by default the persistent partition has a value of 0 # if you want any of the extra partitions to fill the rest of the space you will need to set the persistent partition # size to a different value, for example # partitions: # persistent: # size: 300 # default partitions # only 'oem', 'recovery', 'state' and 'persistent' objects allowed # size in MiB partitions: oem: label: COS_OEM size: 60 fs: ext4 recovery: label: COS_RECOVERY size: 4096 fs: ext4 # extra partitions to create during install # only size, label and fs are used # if no fs is given the partition will be created but not formatted # This partitions are not automounted only created and formatted extra-partitions: - Name: myPartition size: 100 fs: ext4 label: EXTRA_PARTITION - Name: myOtherPartition size: 0 fs: ext4 label: EXTRA_PARTITION # no-format: true skips any disk partitioning and formatting # if set to true installation procedure will error out if expected # partitions are not already present within the disk. no-format: false # if no-format is used and elemental is running over an existing deployment # force cane be used to force installation. force: false # use this iso as installation media (overwrites 'system.uri' and 'recoverys-system.uri' # according to the ISO contents. iso: https://my.domain.org/some/powerful.iso # main OS image # size in MiB system: label: COS_ACTIVE size: 1024 fs: ext2 uri: oci:some.registry.org/elemental/image:latest # recovery OS image recovery-system: fs: squashfs uri: oci:recovery/elemental # filesystem label of the passive backup image passive.label: COS_PASSIVE # extra cloud-init config file URI to include during the installation cloud-init: \"https://some.cloud-init.org/my-config-file\" # grub menu entry, this is the string that will be displayed grub-entry-name: Elemental # tty console to add into the kernel parameters tty: ttyS0 # configuration for the 'reset' command reset: # if set to true it will format persistent partitions ('oem 'and 'persistent') reset-persistent: false reset-oem: false # OS image used to reset disk # size in MiB system: label: COS_ACTIVE size: 1024 fs: ext2 uri: docker:some.registry.org/cos/image:latest # filesystem label of the passive backup image passive.label: COS_PASSIVE # grub menu entry, this is the string that will be displayed grub-entry-name: Elemental # tty console to add into the kernel parameters tty: ttyS0 # configuration used for the 'upgrade' command upgrade: # if set to true upgrade command will upgrade recovery system instead # of main active system recovery: false # image used to upgrade main OS # size in MiB system: uri: oci:system/elemental # image used to upgrade recovery OS # recovery images can be set to use squashfs recovery-system: fs: squashfs uri: oci:recovery/elemental # grub menu entry, this is the string that will be displayed grub-entry-name: Elemental # use cosign to validate images from container registries cosign: true # cosign key to used for validation cosign-key: myKey # attempt a verify process no-verify: false # fail on cloud-init hooks errors strict: false # Additional paths to look for cloud-init files cloud-init-paths: - \"/some/path\" # reboot/power off when done reboot: false poweroff: false Complete source code: https://github.com/rancher/elemental-toolkit/blob/main/config.yaml.example The system and recovery-system objects are an image specification. An image specification is defined by:\nfs: defines the filesystem of the image. Currently only ext2 and squashfs should be used for images and squashfs is only supported for the recovery-system image. label: defines the filesystem label. It is strongly recommended to use default labels as it is easy to fall into inconsistent states when changing labels as all changes should also be reflected in several other parts such as the bootloader configuration. This attribute has no effect for squashfs filesystems. uri: defines the source of the image. The uri must include a valid scheme to identify the type of source. It supports oci, dir and file schemes. size: defines the filesystem image size in MiB, it must be big enough to store the defined image source. This attribute has no effect for squashfs filesystems. The partitions object lists partition specifications. A partition specifications is defined by:\nfs: defines the filesystem of the partition. Currently only ext2, ext4 and xfs are supported being ext4 the default. label: defines the label of the filesystem of the partition. It is strongly recommended to use default labels as it is easy to fall into inconsistent states when changing labels as all changes should also be reflected in several other parts such as the bootloader configuration. size: defines the partition size in MiB. A zero size means use all available disk, obviously this only makes sense for the last partition, the persistent partition. flags: is a list of strings, this is used as additional partition flags that are passed to parted (e.g. boot flag). Defaults should be just fine for most of the cases. ","categories":"","description":"Configuring an Elemental derivative\n","excerpt":"Configuring an Elemental derivative\n","ref":"/docs/customizing/general_configuration/","tags":"","title":"General Configuration"},{"body":"Elemental vanilla images by default are picking upgrades by the standard upgrade channel. It means it will always get the latest published Elemental version by our CI.\nHowever, it’s possible to tweak the default behavior of elemental upgrade to point to a specific OCI image/tag, or a different release channel.\nConfiguration elemental upgrade during start reads the Elemental configuration file and allows to tweak the following:\n# configuration used for the 'ugrade' command upgrade: # if set to true upgrade command will upgrade recovery system instead # of main active system recovery: false # image used to upgrade main OS # size in MiB system: uri: \u003cimage-spec\u003e # image used to upgrade recovery OS # recovery images can be set to use squashfs recovery-system: fs: squashfs uri: oci:recovery/cos The system and recovery-system objects define the OS image used for the main active system and the recovery system respectively. They both are fined by a \u003cimage-spec\u003e.\n","categories":"","description":"Customizing the default upgrade channel\n","excerpt":"Customizing the default upgrade channel\n","ref":"/docs/customizing/upgrades/","tags":"","title":"Upgrades"},{"body":"This is a work in progress example of how to deploy K3S + Fleet + System Upgrade Controller over a Elemental vanilla image only by using cloud-init yaml configuration files. The config file reproduced here is meant to be included as a user-data in a cloud provider (aws, gcp, azure, etc) or as part of a cdrom (Elemental-Recovery will try to fetch /userdata file from a cdrom device).\nA vanilla image is an image that only provides the Elemental-Recovery system on a COS_RECOVERY partition. It does not include any other system and it is meant to be dumped to a bigger disk and deploy a Elemental system or a derivative system over the free space in disk. COS vanilla images are build as part of the CI workflow, see CI artifacts to download one of those.\nThe configuration file of this example has two purposes: first it deploys Elemental, second in reboots on the deployed OS and deploys K3S + Fleet + System Upgrades Controller.\nOn first boot it will fail to boot Elemental grub menu entry and fallback to Elemental-Recovery system. From there it will partition the vanilla image to create the main system partition (COS_STATE) and add an extra partition for persistent data (COS_PERSISTENT). It will use the full disk, a disk of at least 20GiB is recommended. After partitioning it will deploy the main system on COS_STATE and reboot to it.\nOn consequent boots it will simply boot from COS_STATE, there it prepares the persistent areas of the system (arranges few bind mounts inside COS_PERSISTENT) and then it runs an standard installation of K3s, Fleet and System Upgrade Controller. After few minutes after the system is up the K3s cluster is up and running.\nNote this setup similar to the derivative example using Fleet. The main difference is that this example does not require to build any image, it is pure cloud-init configuration based.\nUser data configuration file name: \"Default deployment\" stages: rootfs.after: - if: '[ -f \"/run/cos/recovery_mode\" ]' name: \"Repart image\" layout: # It will partition a device including the given filesystem label or part label (filesystem label matches first) device: label: COS_RECOVERY add_partitions: - fsLabel: COS_STATE # 15Gb for COS_STATE, so the disk should have, at least, 20Gb size: 15360 pLabel: state - fsLabel: COS_PERSISTENT # unset size or 0 size means all available space pLabel: persistent - if: '[ ! -f \"/run/cos/recovery_mode\" ]' name: \"Persistent state\" environment_file: /run/cos/cos-layout.env environment: VOLUMES: \"LABEL=COS_OEM:/oem LABEL=COS_PERSISTENT:/usr/local\" OVERLAY: \"tmpfs:25%\" RW_PATHS: \"/var /etc /srv\" PERSISTENT_STATE_PATHS: \"/root /opt /home /var/lib/rancher /var/lib/kubelet /etc/systemd /etc/rancher /etc/ssh\" network.before: - name: \"Setup SSH keys\" authorized_keys: root: # It can download ssh key from remote places, such as github user keys (e.g. `github:my_user`) - my_custom_ssh_key - if: '[ ! -f \"/run/cos/recovery_mode\" ]' name: \"Fleet deployment\" files: - path: /etc/k3s/manifests/fleet-config.yaml content: | apiVersion: helm.cattle.io/v1 kind: HelmChart metadata: name: fleet-crd namespace: kube-system spec: chart: https://github.com/rancher/fleet/releases/download/v0.3.3/fleet-crd-0.3.3.tgz --- apiVersion: helm.cattle.io/v1 kind: HelmChart metadata: name: fleet namespace: kube-system spec: chart: https://github.com/rancher/fleet/releases/download/v0.3.3/fleet-0.3.3.tgz network: - if: '[ -f \"/run/cos/recovery_mode\" ]' name: \"Deploy cos-system\" commands: # Deploys the recovery image. # use --docker-image to deploy a custom image # e.g. `elemental reset --docker-image quay.io/my_custom_repo:my_image` - elemental reset --reboot - if: '[ ! -f \"/run/cos/recovery_mode\" ]' name: \"Setup k3s\" directories: - path: \"/usr/local/bin\" permissions: 0755 owner: 0 group: 0 commands: - | curl -sfL https://get.k3s.io | \\ INSTALL_K3S_VERSION=\"v1.20.4+k3s1\" \\ INSTALL_K3S_EXEC=\"--tls-san {{.Values.node.hostname}}\" \\ INSTALL_K3S_SELINUX_WARN=\"true\" \\ sh - # Install fleet kubectl apply -f /etc/k3s/manifests/fleet-config.yaml # Install system-upgrade-controller kubectl apply -f https://raw.githubusercontent.com/rancher/system-upgrade-controller/v0.6.2/manifests/system-upgrade-controller.yaml ","categories":"","description":"Running k3s and Fleet on a Elemental vanilla raw image\n","excerpt":"Running k3s and Fleet on a Elemental vanilla raw image\n","ref":"/docs/tutorials/k3s_and_fleet_on_vanilla_image_example/","tags":"","title":"K3s + Fleet"},{"body":"\nIn this tutorial we will:\nBuild a custom OS image to deploy in our cluster Setup a cluster with Elemental, k3s and fleet Upgrade the cluster to our custom OS image with fleet This repository contains the full example code.\n1) Build the OS image # IMAGE=quay.io/costoolkit/test-images:fleet-sample # cd os # docker build -t $IMAGE . 2) Push the docker image # docker push $IMAGE 3) Prepare a Elemental VM Download an ISO, or a qcow image from the Github artifacts of Elemental.\nIf deploying on AWS/openstack/Cloud, use the fleet-cloud-init.yaml file as userdata. If deploying on baremetal/VMs, place fleet-cloud-init.yaml in /oem after install (or run the installer with elemental install --cloud-init https://raw.githubusercontent.com/rancher-sandbox/cos-fleet-upgrades-sample/main/fleet-cloud-init.yaml $DEVICE).\nReboot, after some bootstraping time (check until all pods are running with watch kubectl get pods -A), you should have a k3s cluster with fleet and system-upgrade-controller deployed.\n4) Upgrade with fleet Add your fleet repository to the fleet cluster:\ncat \u003e example.yaml \u003c\u003c \"EOF\" apiVersion: fleet.cattle.io/v1alpha1 kind: GitRepo metadata: name: upgrade # This namespace is special and auto-wired to deploy to the local cluster namespace: fleet-local spec: # Everything from this repo will be ran in this cluster. You trust me right? repo: \"https://github.com/rancher-sandbox/cos-fleet-upgrades-sample\" branch: \"main\" paths: - manifests EOF kubectl apply -f example.yaml An example of how to trigger an upgrade with fleet is in manifests/upgrade.yaml. Edit the image with the one generated in the previous steps, and commit it to your fleet repository, At this point you should see the upgrade job to kick-in, the system will reboot afterwards.\n","categories":"","description":"Using fleet to trigger upgrades on Elemental-based derivatives\n","excerpt":"Using fleet to trigger upgrades on Elemental-based derivatives\n","ref":"/docs/tutorials/trigger_upgrades_with_fleet/","tags":"","title":"Trigger upgrades with K3s and Fleet"},{"body":"You can find here examples on how to tweak a system via cloud-config various aspects of an Elemental-toolkit derivative.\nThe examples are meant to be placed as yaml files under /oem ore either /usr/local/cloud-config. They can be also given as input cloud-config while calling elemental install.\nNetworking with NetworkManager By default all interfaces will get automatically an IP address, however, there are situations where a static IP is desired, or custom configuration to be specified, here you can find some network settings with NetworkManager.\nAdditional NIC Set static IP to an additional NIC:\nname: \"Default network configuration\" stages: boot: - commands: - nmcli dev up eth1 - name: \"Setup network\" files: - path: /etc/sysconfig/network/ifcfg-eth1 content: | BOOTPROTO='static' IPADDR='192.168.1.2/24' permissions: 0600 owner: 0 group: 0 Static IP Set static IP to default interface:\nname: \"Default network configuration\" stages: boot: - commands: - nmcli dev up eth0 initramfs: - name: \"Setup network\" files: - path: /etc/sysconfig/network/ifcfg-eth0 content: | BOOTPROTO='static' IPADDR='192.168.1.2/24' permissions: 0600 owner: 0 group: 0 DHCP name: \"Default network configuration\" stages: boot: - commands: - nmcli dev up eth1 initramfs: - name: \"Setup network\" files: - path: /etc/sysconfig/network/ifcfg-eth1 content: | BOOTPROTO='dhcp' STARTMODE='onboot' permissions: 0600 owner: 0 group: 0 Additional files K3s manifests Add k3s manifests:\nname: \"k3s\" stages: network: - if: '[ ! -f \"/run/cos/recovery_mode\" ]' name: \"Fleet deployment\" files: - path: /var/lib/rancher/k3s/server/manifests/fleet-config.yaml content: | apiVersion: v1 kind: Namespace metadata: name: cattle-system --- apiVersion: helm.cattle.io/v1 kind: HelmChart metadata: name: fleet-crd namespace: cattle-system spec: chart: https://github.com/rancher/fleet/releases/download/v0.3.8/fleet-crd-0.3.8.tgz --- apiVersion: helm.cattle.io/v1 kind: HelmChart metadata: name: fleet namespace: cattle-system spec: chart: https://github.com/rancher/fleet/releases/download/v0.3.8/fleet-0.3.8.tgz ","categories":"","description":"Useful copy-paste cloud-config settings\n","excerpt":"Useful copy-paste cloud-config settings\n","ref":"/docs/examples/cloud_config/","tags":"","title":"Cloud config examples"},{"body":"elemental-toolkit is a manifest to share a common abstract layer between derivatives inheriting the same featureset.\nHigh level workflow The building workflow can be resumed in the following steps:\nBuild a container image (docker build / nerdctl build / buildah ..) Publish the image (docker push / nerdctl push ) Build an ISO (elemental build-iso) Boot a machine using the ISO and run installation (elemental install) Reboot into the installed system While on the client side, the upgrade workflow is:\nelemental upgrade --system.uri=oci:\u003cimage:version\u003e Single image OS Derivatives are composed by a combination of specs to form a single image OS.\nThe container image during installation and upgrade, is converted to an image file with a backing ext2 fs.\nBuild ISO To build an iso for a derivative image elemental build-iso command can be used:\nelemental build-iso -n $NAME $SOURCE Where $NAME is the name of the ISO and $SOURCE might be the reference to the directory, file, container image or chaneel we are building the ISO for. $SOURCE should be provided as uri in following format :, where: * - might be [“dir”, “file”, “oci”, “docker”], as default is taken “oci” * - is path to file or directory, channel or image name with tag version (if tag was not provided then “latest” is used)\nSome examples for $SOURCE argument “dir:/cOS/system”, “oci:quay.io/repository/costoolkit/releases-green:cos-system-0.8.14-10”\nSee also building ISOs\n","categories":"","description":"This document summarize references to create derivatives with `elemental-toolkit`.\n","excerpt":"This document summarize references to create derivatives with …","ref":"/docs/development/creating_derivatives/","tags":"","title":"Creating derivatives"},{"body":"This guide will guide in a step-by-step process to build a derivative which is fully compatible with Elemental, and will illustrate how to make customization on such image, by adding for example a default set of services and a custom user.\nThe derivative will be based on openSUSE and embed k3s, and a custom user joe which will be already set to allow us to login.\nPrerequisites Docker installed 1) Create a Dockerfile Let’s create a workspace directory and move into it:\n$\u003e mkdir derivative $\u003e cd derivative Let’s create now a Dockerfile for our image inside that directory, which will be represent running system:\nFROM ghcr.io/rancher/elemental-toolkit/elemental-cli:v0.11.0 AS elemental FROM registry.suse.com/suse/sle-micro-rancher/5.2 ARG K3S_VERSION=v1.20.4+k3s1 ARG ARCH=amd64 ENV ARCH=${ARCH} COPY --from=elemental /install-root / COPY --from=elemental /usr/bin/elemental /usr/bin/elemental # Install k3s server/agent ENV INSTALL_K3S_VERSION=${K3S_VERSION} RUN curl -sfL https://get.k3s.io \u003e installer.sh \u0026\u0026 \\ INSTALL_K3S_SKIP_START=\"true\" INSTALL_K3S_SKIP_ENABLE=\"true\" sh installer.sh \u0026\u0026 \\ INSTALL_K3S_SKIP_START=\"true\" INSTALL_K3S_SKIP_ENABLE=\"true\" sh installer.sh agent \u0026\u0026 \\ rm -rf installer.sh ## System layout # Required by k3s etc. RUN mkdir /usr/libexec \u0026\u0026 touch /usr/libexec/.keep # Copy custom files # COPY files/ / # Copy cloud-init default configuration COPY cloud-init.yaml /system/oem/ # Generate initrd RUN elemental init --force # OS level configuration RUN echo \"VERSION=999\" \u003e /etc/os-release RUN echo \"GRUB_ENTRY_NAME=derivative\" \u003e\u003e /etc/os-release RUN echo \"welcome to our derivative\" \u003e\u003e /etc/issue.d/01-derivative # Copy cloud-init default configuration COPY cloud-init.yaml /system/oem/ 2) Configuration At the end of the Dockerfile, you can see that we copy over a custom cloud-init file:\n# Copy cloud-init default configuration COPY cloud-init.yaml /system/oem/ Create a cloud-init.yaml file as the derivative/cloud-init.yaml with the following content:\n# See https://rancher.github.io/elemental-toolkit/docs/reference/cloud_init/ for a full syntax reference name: \"Default settings\" stages: initramfs: # Setup default hostname - name: \"Branding\" hostname: \"derivative\" # Setup an admin group with sudo access - name: \"Setup groups\" ensure_entities: - entity: | kind: \"group\" group_name: \"admin\" password: \"x\" gid: 900 # Setup network - openSUSE specific - name: \"Network setup\" files: - path: /etc/sysconfig/network/ifcfg-eth0 content: | BOOTPROTO='dhcp' STARTMODE='onboot' permissions: 0600 owner: 0 group: 0 # Setup a custom user - name: \"Setup users\" users: # Replace the default user name here and settings joe: # Comment passwd for no password passwd: \"joe\" shell: /bin/bash homedir: \"/home/joe\" groups: - \"admin\" #authorized_keys: # Replace here with your ssh keys # joe: # - ssh-rsa .... # Setup sudo - name: \"Setup sudo\" files: - path: \"/etc/sudoers\" owner: 0 group: 0 permsisions: 0600 content: | Defaults always_set_home Defaults secure_path=\"/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/bin:/usr/local/sbin\" Defaults env_reset Defaults env_keep = \"LANG LC_ADDRESS LC_CTYPE LC_COLLATE LC_IDENTIFICATION LC_MEASUREMENT LC_MESSAGES LC_MONETARY LC_NAME LC_NUMERIC LC_PAPER LC_TELEPHONE LC_ATIME LC_ALL LANGUAGE LINGUAS XDG_SESSION_COOKIE\" Defaults !insults root ALL=(ALL) ALL %admin ALL=(ALL) NOPASSWD: ALL @includedir /etc/sudoers.d commands: - passwd -l root # Setup persistency so k3s works properly # See also: https://rancher.github.io/elemental-toolkit/docs/reference/immutable_rootfs/#configuration-with-an-environment-file rootfs.after: - name: \"Immutable Layout configuration\" environment_file: /run/cos/cos-layout.env environment: VOLUMES: \"LABEL=COS_OEM:/oem LABEL=COS_PERSISTENT:/usr/local\" OVERLAY: \"tmpfs:25%\" RW_PATHS: \"/var /etc /srv\" PERSISTENT_STATE_PATHS: \u003e- /etc/systemd /etc/rancher /etc/ssh /etc/iscsi /etc/cni /home /opt /root /usr/libexec /var/log /var/lib/rancher /var/lib/kubelet /var/lib/NetworkManager /var/lib/longhorn /var/lib/cni PERSISTENT_STATE_BIND: \"true\" # Finally, let's start k3s when network is available, and download the SSH key from github for the joe user network: - name: \"Start k3s\" systemctl: start: - k3s - authorized_keys: # Replace here with your ssh keys or github handle joe: - github:joe Done! We are now ready to build the container image.\nThe file structure should be like the following:\n$\u003e tree ./derivative derivative ├── cloud-init.yaml ├── Dockerfile ├── iso.yaml └── repositories.yaml 3) Build it! Now we are ready to build our docker image:\n$~/derivative\u003e docker build -t derivative:latest . ... ---\u003e Running in a9c33b42f567 Removing intermediate container a9c33b42f567 ---\u003e 8e83191d29df Step 19/19 : COPY cloud-init.yaml /system/oem/ ---\u003e 38cc4c8b173a Successfully built 38cc4c8b173a Successfully tagged derivative:latest After the process completed, we are ready to consume our docker image. If you push the image over a container registry, you can then or use a running Elemental system to upgrade to it, or deploy it directly see getting started.\nBuild an ISO image We can at this point also create a ISO from it.\nCreate a manifest.yaml file with the following content inside the derivative folder:\niso: bootloader-in-rootfs: true grub-entry-name: \"Derivative Installer\" squash-no-compression: true Now, we can build the ISO with:\n$~/derivative\u003e elemental build-iso --config-dir=./ derivative:latest .... INFO[0114] Copying BIOS kernels INFO[0114] Create squashfs Parallel mksquashfs: Using 8 processors Creating 4.0 filesystem on /tmp/elemental-geniso4082786464/tempISO/rootfs.squashfs, block size 1048576. .... INFO[0247] 🍹 Generate ISO derivative-0.20210909.iso xorriso 1.4.6 : RockRidge filesystem manipulator, libburnia project. Drive current: -outdev 'stdio:derivative-0.20210909.iso' Media current: stdio file, overwriteable Media status : is blank Media summary: 0 sessions, 0 data blocks, 0 data, 448g free Added to ISO image: directory '/'='/tmp/elemental-geniso4082786464/tempISO' xorriso : UPDATE : 599 files added in 1 seconds xorriso : UPDATE : 599 files added in 1 seconds xorriso : NOTE : Copying to System Area: 512 bytes from file '/tmp/elemental-geniso4082786464/tempISO/boot/x86_64/loader/boot_hybrid.img' xorriso : WARNING : Boot image load size exceeds 65535 blocks of 512 bytes. Will record 0 in El Torito to extend ESP to end-of-medium. libisofs: NOTE : Aligned image size to cylinder size by 137 blocks xorriso : UPDATE : 12.35% done xorriso : UPDATE : 42.73% done ISO image produced: 282624 sectors Written to medium : 282624 sectors at LBA 0 Writing to 'stdio:derivative-0.20210909.iso' completed successfully. After the process completes, we should have a ISO in our folder ready to be used. See the build ISOs section for all the available options.\nCustomization Here follows a break down of the steps above\nAdding packages Feel free to edit the Dockerfile with the packages you want to embed in our image. You can install any packages available in the openSUSE repositories by tweaking\n# Install packages from the base image RUN zypper in -y \\ .... # Add more packages here! System layout and k3s We set some default layouts and install k3s:\n# Install k3s server/agent ENV INSTALL_K3S_VERSION=${K3S_VERSION} RUN curl -sfL https://get.k3s.io \u003e installer.sh \u0026\u0026 \\ INSTALL_K3S_SKIP_START=\"true\" INSTALL_K3S_SKIP_ENABLE=\"true\" sh installer.sh \u0026\u0026 \\ INSTALL_K3S_SKIP_START=\"true\" INSTALL_K3S_SKIP_ENABLE=\"true\" sh installer.sh agent \u0026\u0026 \\ rm -rf installer.sh ## System layout # Required by k3s etc. RUN mkdir /usr/libexec \u0026\u0026 touch /usr/libexec/.keep # Copy custom files # COPY files/ / # Generate initrd RUN elemental init --force # OS level configuration RUN echo \"VERSION=999\" \u003e /etc/os-release RUN echo \"GRUB_ENTRY_NAME=derivative\" \u003e\u003e /etc/os-release RUN echo \"welcome to our derivative\" \u003e\u003e /etc/issue.d/01-derivative As our target here is to install k3s we do install both k3s agent and server, so the image can work in both modes. Here we could have installed any service or binary that we want to embed in our container image. We setup the system layout by creating needed paths for k3s and set up a os-release which identifies the OS version. Afterward we regenerate the initrd which is required in order to boot, see also the Initrd section.\nCloud-init, custom SSH access The cloud-init.yaml file above configures a user, joe and attaches a ssh key to it in order to login. It also sets up a default password, which is optional.\nTo specify any additional ssh key installed within the user, we do:\nnetwork: - authorized_keys: joe: - github:joe which you can replace with your github handle, or by specifying directly an ssh key. In case you specify the SSH key directly, you don’t need to run the step in the network stage.\nThe user will be part of the admin group which is allowed to use sudo.\nAs our target is to run k3s, but could have been any other service, we tweak the immutable setup by specifying sensible path required for k3s in order to function properly, see immutable rootfs for all the available options.\nFinally, we start k3s. Note, we could have tweaked that part slightly to provide k3s configurations via systemd env files, or boot up for example the agent instead of the server:\nnetwork: - if: '[ ! -f \"/run/cos/recovery_mode\" ]' name: \"Setup k3s\" # Setup environment file for custom k3s arguments environment_file: \"/etc/systemd/system/k3s.service.env\" environment: FOO: \"bar\" systemctl: start: - k3s - commands: - | chmod 600 /etc/systemd/system/k3s.service.env ","categories":"","description":"This document is a step-by-step guide to build a customized embedded system that can be used in Elemental\n","excerpt":"This document is a step-by-step guide to build a customized embedded …","ref":"/docs/examples/embedded_images/","tags":"","title":"Creating embedded images"},{"body":"","categories":"","description":"This section contains various articles relative on how to customize Elemental, branding and behavior.\n","excerpt":"This section contains various articles relative on how to customize …","ref":"/docs/customizing/","tags":"","title":"Customizing"},{"body":"Requirements:\nqemu-img utility elemental binary elemental runtime dependencies The suggested approach is based on using the Elemental installer (elemental install command) to run the installation from a Linux to a loop device. The loop device can be a raw image created with qemu-img create that can easily be converted to other formats after the installation by using qemu-img convert.\nPrepare the loop device Preparing the a loop device for the installation is simple and straight forward.\n# Create a raw image of 32G \u003e qemu-img create -f raw disk.img 32G # Set the disk image as a loop device \u003e sudo losetup -f --show disk.img \u003cdevice\u003e Run elemental installation Execute the elemental installation as described in installing:\n\u003e sudo elemental install --firmware efi --system.uri oci:\u003cimage=ref\u003e \u003cdevice\u003e Where \u003cimage-ref\u003e is the Elemental derivative container image we want to use for the disk creation and \u003cdevice\u003e is the loop device previously created with losetup (e.g. /dev/loop0).\nConvert the RAW image to desired format Once the installation is done just unsetting the loop device and converting the image to the desired format is missing:\n# Unset the loop device \u003e sudo losetup -d \u003cdevice\u003e # Convert the RAW image to qcow2 \u003e qemu-img convert -f raw -O qcow2 disk.img disk.qcow2 QEMU supports a wide range of formats including common ones such as vdi, vmdk or vhdx.\nThe result can be easily tested on QEMU with:\n\u003e qemu -m 4096 -hda disk.qcow2 -bios /usr/share/qemu/ovmf-x86_64.bin Note the firmware image path varies depending on the host distro, the path provided in this example is based on openSUSE Leap.\n","categories":"","description":"This section documents the procedure to build disk images using elemental\n","excerpt":"This section documents the procedure to build disk images using …","ref":"/docs/creating-derivatives/build_disk/","tags":"","title":"Build disk images with Elemental"},{"body":"In order to build an ISO we rely on elemental build-iso command. It accepts a YAML file denoting the sources to bundle in an ISO. In addition it can also overlay custom files or use container images from a registry as packages.\nTo build an ISO, just run:\ndocker run --rm -ti -v $(pwd):/build ghcr.io/rancher/elemental-toolkit/elemental-cli:latest --debug build-iso -o /build $SOURCE Where $SOURCE might be the container image you want to build the ISO for, you might want to check on how to build bootable images. Argument $SOURCE might be the reference to the directory, file, container image or channel we are building the ISO for, it should be provided as uri in following format :, where: * - might be [“dir”, “file”, “oci”, “docker”], as default is taken “docker” * - is path to file or directory, channel or image name with tag version (if tag was not provided then “latest” is used)\nelemental build-iso command also supports reading a configuration manifest.yaml file. It is loaded form the directory specified by --config-dir elemental’s flag.\nAn example of a yaml file using the bootloader from the contained image:\niso: bootloader-in-rootfs: true grub-entry-name: \"Installer\" name: \"Elemental-0\" date: true What’s next? Check out on how to build an image from the ISO we have just created Syntax Below you can find a full reference about the yaml file format.\niso: # Sources to be installed in the rootfs rootfs: - .. # Sources to be installed in the uefi image uefi: - .. # Sources to be installed in the iso image image: - .. label: \"COS_LIVE\" Sources can be an image reference (then an explicit tag is required) or a local path. Sources are stacked in the given order, so one can easily overwrite or append data by simply adding a local path as the last source.\nCommand flags name: Name of the ISO image. It will be used to generate the *.iso file name output: Path of the destination folder of created images date: If present it includes the date in the generated file name overlay-rootfs: Sets the path of a tree to overlay on top of the system root-tree overlay-uefi: Sets the path of a tree to overaly on top of the EFI image root-tree overlay-iso: Sets the path of a tree to overlay on top of the ISO filesystem root-tree label: Sets the volume label of the ISO filesystem Configuration reference iso.rootfs A list of sources in uri format (container image or local path) [ “docker”, “oci”, “dir”, “file” ] to install in the rootfs. The rootfs will be squashed to a rootfs.squashfs file\niso.uefi A list of sources in uri format (container image or local path) [ “docker”, “oci”, “dir”, “file” ] to install in the efi FAT image or partition.\niso.image A list of sources in uri format (container image or local path) [ “docker”, “oci”, “dir”, “file” ] to install in ISO filesystem.\niso.label The label of the ISO filesystem. Defaults to COS_LIVE. Note this value is tied with the bootloader and kernel parameters to identify the root device.\nname A string representing the ISO final image name without including the .iso\ndate Boolean indicating if the output image name has to contain the date\noutput Folder destination of the built artifacts. It attempts to create if it doesn’t exist.\nCustomize bootloader with GRUB Boot menu and other bootloader parameters can then be easily customized by using the overlay parameters within the ISO config yaml manifest.\nAssuming the ISO being built includes:\niso: rootfs: - ... uefi: - oci:example-grub2-efi-image:latest image: - oci:example-grub2:latest - oci:example-grub2-efi-image:latest We can customize either the image packages (in the referrence image live/grub2 package includes bootloader configuration) or make use of the overlay concept to include or overwrite addition files for image section.\nConsider the following example:\niso: rootfs: - ... uefi: - oci:example-grub2-efi-image:latest image: - oci:example-grub2:latest - oci:example-grub2-efi-image:latest - dir:/my/path/to/overlay/iso With the above the ISO will also include the files under /my/path/to/overlay/iso path. To customize the boot menu parameters consider copy and modify relevant files from example-grub2:latest image. In this example the overlay folder files list could be:\n# image files for grub2 boot boot/grub2/grub.cfg Being boot/grub2/grub.cfg a custom grub2 configuration including custom boot menu entries. Consider the following grub.cfg example:\nsearch --file --set=root /boot/kernel.xz set default=0 set timeout=10 set timeout_style=menu set linux=linux set initrd=initrd if [ \"${grub_cpu}\" = \"x86_64\" -o \"${grub_cpu}\" = \"i386\" ];then if [ \"${grub_platform}\" = \"efi\" ]; then set linux=linuxefi set initrd=initrdefi fi fi set font=($root)/boot/x86_64/loader/grub2/fonts/unicode.pf2 if [ -f ${font} ];then loadfont ${font} fi menuentry \"Custom grub2 menu entry\" --class os --unrestricted { echo Loading kernel... $linux ($root)/boot/kernel.xz cdroot root=live:CDLABEL=COS_LIVE rd.live.dir=/ rd.live.squashimg=rootfs.squashfs console=tty1 console=ttyS0 rd.cos.disable echo Loading initrd... $initrd ($root)/boot/rootfs.xz } Separate recovery To make an ISO with a separate recovery image as squashfs, you can either use the default from Elemental, by adding it in the iso yaml file:\niso: rootfs: .. uefi: .. image: ... - oci:example-recovery:latest The installer will detect the squashfs file in the iso, and will use it when installing the system. You can customize the recovery image as well by providing your own.\n","categories":"","description":"Build ISOs from bootable images\n","excerpt":"Build ISOs from bootable images\n","ref":"/docs/creating-derivatives/build_iso/","tags":"","title":"Build ISOs"},{"body":"A derivative is a standard container image which can be booted by Elemental.\nWe can identify a build phase where we build the derivative, and a “runtime phase” where we consume it.\nThe image is described by a Dockerfile, composed of a base OS of choice (e.g. openSUSE, Ubuntu, etc. ) and the Elemental toolkit itself in order to be consumed by Elemental and allow to be upgraded from by other derivatives.\nelemental-toolkit then converts the OCI artifact into a bootable medium (ISO, packer, ova, etc) and the image itself then can be used to bootstrap other derivatives, which can in turn upgrade to any derivative built with Elemental.\nA derivative can also be later re-used again as input as base-image for downstream derivatives.\nAll the documentation below imply that the container image generated will be the booting one, there are however several configuration entrypoint that you should keep in mind while building the image which are general across all the implementation:\nCustom persistent runtime configuration has to be provided in /system/oem for derivatives, see also the documentation section. Everything under /system/oem will be loaded during the various stages (boot, network, initramfs). /etc/cos/bootargs.cfg contains the booting options required to boot the image with GRUB, see grub customization Derivatives inherits Elemental defaults, which you can override during the build process, however there are some defaults which are relevant and listed below:\n","categories":"","description":"Documents various methods for creating Elemental derivatives\n","excerpt":"Documents various methods for creating Elemental derivatives\n","ref":"/docs/creating-derivatives/","tags":"","title":"Creating derivatives"},{"body":"The immutable rootfs concept in Elemental is provided by a dracut module. By default, elemental and derivatives will inherit an immutable setup.\nA running system will look like as follows:\n/usr/local - persistent (COS_PERSISTENT) /oem - persistent (COS_OEM) /etc - ephemeral /usr - read only / immutable This means that any changes that are not specified as cloud-init configuration are not persisting across reboots.\nYou can place persisting cloud-init files either in /oem or /usr/local/oem, Elemental already supports cloud-init datasources, so you can use also load cloud-init configuration as standard userdata, depending on the platform. For more details on the cloud-init syntax, see the cloud-init configuration reference.\n","categories":"","description":"Immutable root filesystem configuration parameters\n","excerpt":"Immutable root filesystem configuration parameters\n","ref":"/docs/reference/immutable_rootfs/","tags":"","title":"Immutable Root Filesystem"},{"body":"Elemental is set to deploy a persistent grub.cfg into the COS_STATE partition during the system installation or image creation. Elemental grub configuration includes three menu entries: first for the main OS system, second for the fallback OS system and a third for the recovery OS.\nFor example the main OS system menu entry could be something like:\nmenuentry \"Elemental\" --id elemental { search.fs_label COS_STATE root set img=/cOS/active.img set label=COS_ACTIVE loopback loop0 /$img set root=($root) source (loop0)/etc/cos/bootargs.cfg linux (loop0)$kernel $kernelcmd initrd (loop0)$initramfs } Kernel parameters The kernel parameters are not part of the persistent grub.cfg file stored in COS_STATE partition. Kernel parameters are sourced from the loop device of the OS image to boot. This is mainly to keep kernel parameters consistent across different potential OS images or system upgrades. Specifying default custom boot options Elemental images and its derivatives, are expected to include a /etc/cos/bootargs.cfg file which provides the definition of the following variables:\n$kernel: Path of the kernel binary $kernelcmd: Kernel parameters $initramfs: Path of the initrd binary This is the mechanism any Elemental image or Elemental derivative has to communicate its boot parameters (kernel, kernel params and initrd file) to GRUB2.\nFor example, the default Elemental bootarg.cfg file is:\nset kernel=/boot/vmlinuz if [ -n \"$recoverylabel\" ]; then # Boot arguments when the image is used as recovery set kernelcmd=\"console=tty1 root=live:CDLABEL=$recoverylabel rd.live.dir=/ rd.live.squashimg=$img panic=5\" else # Boot arguments when the image is used as active/passive set kernelcmd=\"console=tty1 root=LABEL=$label iso-scan/filename=$img panic=5 security=selinux rd.cos.oemlabel=COS_OEM selinux=1\" fi set initramfs=/boot/initrd You can tweak that file to suit your needs if you need to specify persistent boot arguments.\nNote rd.cos.oemlabel=COS_OEM is required inside the bootargs in order to access to the /oem mount within the rootfs stage. COS_OEM is the default label for the oem partition. Grub environment variables Elemental (since v0.5.8) makes use of the GRUB2 environment block which can used to define persistent GRUB2 variables across reboots.\nUse grub2-editenv command line utility to define the desired values.\nVariable Description next_entry Set the next reboot entry saved_entry Set the default boot entry default_menu_entry Set the name entries on the GRUB menu extra_active_cmdline Set additional boot commands when booting into active extra_passive_cmdline Set additional boot commands when booting into passive extra_recovery_cmdline Set additional boot commands when booting into recovery extra_cmdline Set additional boot commands for all entries default_fallback Sets default fallback logic For instance use the following command to reboot to recovery system only once:\n\u003e grub2-editenv /oem/grubenv set next_entry=recovery Note The examples below make use of the COS_STATE device, only files in the state and oem partitions will be used when booting. Default boot entry The default grub configuration loads the /grubenv of the COS_OEM partition and evaluates on next_entry variable and saved_entry variable. By default none is set.\nThe default boot entry is set to the value of saved_entry, in case the variable is not set grub just defaults to the first menu entry.\nnext_entry variable can be used to overwrite the default boot entry for a single boot. If next_entry variable is set this is only being used once, GRUB2 will unset it after reading it for the first time. This is helpful to define the menu entry to reboot to without having to make any permanent config change.\nUse grub2-editenv command line utility to define desired values.\nFor instance use the following command to reboot to recovery system only once:\n\u003e grub2-editenv /oem/grubenv set next_entry=recovery Or to set the default entry to fallback system:\n\u003e grub2-editenv /oem/grubenv set saved_entry=fallback Boot menu By default Elemental and derivatives shows the default boot menu entry while booting (Elemental).\nThe grub menu entry is generated during installation and can be configured by setting GRUB_ENTRY_NAME in the /etc/os-release file inside the derivative, or either via the general configuration to specify installation details.\nFor example, specifying in /etc/elemental/config.yaml:\ninstall: ... grub-entry-name: myOS ... will automatically set the GRUB menu entries for active, passive and recovery to the specified value.\nThe grub menu boot entry can also be set with grub2-editenv:\n\u003e grub2-editenv /oem/grubenv set default_menu_entry=fooOS Additional menu entries Since\nsystem/grub2-config = 0.0.3-16 it is possible to add multiple custom menu entries to GRUB by creating a /grubcustom config file in the state partition during boot. The grubcustom file will be sourced at the end of the boot process, and can contain several menuentry blocks.\nPersistent boot option flags It is possible to define persistent boot flag for each menu entry also via grub2-editenv:\nextra_active_cmdline: extra bootflags to be applied only on active boot extra_passive_cmdline: extra bootflags to be applied only on passive boot extra_recovery_cmdline: extra bootflags to be applied only on recovery extra_cmdline: will be applied to each boot entry Renaming partition labels During boot the GRUB2 configuration is set to load the grub_oem_env file from the state partition. In this file the following variables are set in order to find system partitions:\nstate_label: Label of state partition. active_label: Filesystem label of active image. passive_label: Filesystem label of passive image. recovery_label: Label of recovery partition. system_label: Filesystem label of recovery image. oem_label: Label of OEM partition. persistent_label: Label of persistent partition. Customizing fallback logic By default Elemental boots into active, and if there are failures will boot into the passive, and finally if keeps failing, will boot into recovery.\nIt is possible to override the default fallback logic by setting default_fallback as grub environment, consider for example:\n\u003e grub2-editenv /oem/grubenv set default_fallback=\"2 0 1\" Will set the default fallback to “2 0 1” instead of the default “0 1 2”.\n","categories":"","description":"GRUB 2 Configuration\n","excerpt":"GRUB 2 Configuration\n","ref":"/docs/customizing/configure_grub/","tags":"","title":"GRUB"},{"body":"Elemental includes basic support for SELinux. From an elemental perspective SELinux is some custom configurationt that requires special treatment. Being specific it mostly nails down to apply SELinux labels at install and upgrade time. Since the rootfs is readonly they can’t be easily applied at runtime or at boot time. As consequence of that SELinux autorelabel service should not be used within elemental as it expects a RW root and persistency across reboots (it essentially reboots after appliying labels). For the time being Elemental only considers the targeted SELinux policy.\nelemental-cli utility applies SELinux contexts to the installed/upgraded system if three conditions are met:\nthe installed system includes the setfiles command the installed system includes the targeted files context (/etc/selinux/targeted/contexts/files/file_contexts file) the binary for targeted policy is also present (/etc/selinux/targeted/policy/policy.* file) In an Elemental workflow SElinux context labels should be applied at install/upgrade time for the readonly areas, but this is not enough as it doesn’t cover the ephemeral filesystems (overlayfs on top of tmpfs), which are usually sensitive paths like /etc/, /var, /srv, etc. In order to properly apply file contexts over the ephemeral paths the relabelling has to happen at boot time once those overlayfs are created. The appropriate stage for that is in initrd before switching root. In fact, it can be done as a cloud-init step as part of the initramfs stage, using the packaged 10_selinux.yaml with:\n# Default Elemental OEM configuration file # # This file is part of Elemental and will get reset during upgrades. # # Before you change this file manually, # consider copying this file to /usr/local/cloud-config or # copy the file with a prefix starting by 90, e.g. /oem/91_custom.yaml name: \"SELinux\" stages: initramfs: - name: \"Relabelling\" commands: - | if grep -q \"selinux=1\" /proc/cmdline; then load_policy -i restorecon -R -i -v /etc /root /opt /srv /var /home /usr/local /oem fi Complete source code: https://github.com/rancher/elemental-toolkit/blob/main/pkg/features/embedded/cloud-config-defaults/system/oem/10_selinux.yaml Note it is required to load the policy in advance to be capable to apply the restorecon command. The restorecon command should be applied to all ephemeral paths and, depending on the specific use case, to the persistent paths too. Note that without restoring context on the ephemeral /etc it is unlikely the system is capable of properly booting, hence this is a very important step if SELinux is intended to used.\nUsing custom SELinux modules Making use of selinux and including SELinux utilities and targeted policy within the base OS it is enough to get started with SELinux, however there is a great chance that this is too generic and requires some additional policy modules to be fully functional according to each specific use case.\nThe Type Enforcement file was created by booting an Elemental OS on permissive mode using audit2allow and other SELinux related utilities to generate the custom module out of the reported denials. Something like:\n# Create the type enforcement file cat /var/log/audit/audit.log | audit2allow -m elemental \u003e elemental.te # Create the policy module checkmodule -M -m -o elemental.mod elemental.te # Create the policy package out of the module semodule_package -o elemental.pp -m elemental.mod To make effective the policy package it has to be loaded or installed within the selinux policy, this can be easily done with the semodule -i /usr/share/elemental/selinux/elemental.pp command. So from a derivative perspective and following the example from Creating bootable image section adding the following lines to the Dockerfile should be enough to enable SELinux in enforcing mode:\n# Install the custom policy package if any and the restore context stage in cloud-init config RUN elemental init --force --features=cloud-config # Load the policy package RUN semodule -i /usr/share/elemental/selinux/elemental.pp # Enable selinux in enforcing mode RUN sed -i \"s|^SELINUX=.*|SELINUX=enforcing|g\" /etc/selinux/config The above assumes the base image already includes the SELinux packages and utilities provided by the underlaying distro. It is suggested to set the enforcing mode via the config file rather than setting grub with the selinux kernel parameter (enforcing=1), this way it is easier, at any time, to temporarily add enforcing=0 at runtime within the grub2 shell and temporarily set SELinux in permissive mode.\nNotes when using a SELinux version prior to v3.4. If libsemanage version is lower than v3.4, it is likely that the semodule -i *.pp command fails with a cross-device linking issue, this is a known issue upstream and already fixed since v3.4. Command selinux -i \u003cfile\u003e mutates files under /var/lib/selinux/targeted and used to rename some files, this can be tricky when executed inside a container as hardlinks across filesystems are not permitted and this is actually what happens if the overlayfs driver is used. This can be worked around if all the originally mutated files are already modified within the execution layer (so they are part of the upper layer of the overlayfs). So the above specific example could be rewritten as:\n# Install the custom policy package if any and the restore context stage in cloud-init config RUN elemental init --force --features=cloud-config # Artificially modify selinux files to copy them in within the overlyfs and then load the policy package RUN mv /var/lib/selinux/targeted/active /var/lib/selinux/targeted/previous \u0026\u0026\\ cp --link --recursive /var/lib/selinux/targeted/previous /var/lib/selinux/targeted/active \u0026\u0026\\ semodule -i /usr/share/elemental/selinux/elemental.pp # Enable selinux in enforcing mode RUN sed -i \"s|^SELINUX=.*|SELINUX=enforcing|g\" /etc/selinux/config ","categories":"","description":"SELinux support in Elemental\n","excerpt":"SELinux support in Elemental\n","ref":"/docs/customizing/selinux_support/","tags":"","title":"SELinux Support"},{"body":"This section describes the runtime layout of a derivative (or a Elemental Vanilla image) once booted in a system.\nThe Elemental toolkit performs during installation a common setup which is equivalent across all derivatives.\nThis mechanism ensures that a layout:\nit’s simple and human friendly allows to switch easily derivatives allows to perform recovery tasks is resilient to upgrade failures Layout The basic setup consists of:\nan A/B partitioning style. We have an ‘active’ and a ‘passive’ system too boot from in case of failures a Recovery system which allows to perform emergency tasks in case of failure of the ‘A/B’ partitions a Fallback mechanism that boots the partitions in this sequence: “A -\u003e B -\u003e Recovery” in case of booting failures The upgrade happens in a transition image and take places only after all the necessary steps are completed. An upgrade of the ‘A/B’ partitions can be done by booting into them and running elemental upgrade. This will create a new pristine image that will be selected as active for the next reboot, the old one will be flagged as passive. If we are performing the same from the passive system, only the active is subject to changes.\nSimilarly, a recovery system can be upgraded as well by running elemental upgrade --recovery. This will upgrade the recovery system instead of the active/passive. Note both commands needs to be run inside the active or passive system.\nPartitions The default partitioning is created during installation and is expected to be present in a booted Elemental system:\na COS_STATE partition that will contain our active, passive and recovery images. The images are located under the /cOS directory a COS_PERSISTENT partition which contains the persistent user data. This directory is mounted over /usr/local during runtime a COS_OEM partition which contains the cloud-init oem files, which is mounted over /oem during runtime a COS_RECOVERY partition which contains the recovery system image The COS_STATE partitions contains the active, passive . While the active and passive are .img files which are loopback mounted, the recovery system is in COS_RECOVERY and can also be a squashfs file (provided in /cOS/recovery.squashfs). This ensures the immutability aspect and ease out building derivative in constrained environments (e.g. when we have restricted permissions and we can’t mount).\nFor more information about the immutability aspect of Elemental, see Immutable rootfs\n","categories":"","description":"Runtime layout of a booted Elemental derivative\n","excerpt":"Runtime layout of a booted Elemental derivative\n","ref":"/docs/reference/layout/","tags":"","title":"Runtime layout"},{"body":"Elemental and every derivative can upgrade, rollback or just switch to different versions in runtime by using the toolkit installed inside the image.\nTo upgrade an installed system, just run elemental upgrade and reboot.\nThis will perform an upgrade based on the default derivative configuration for the image. See general configuration on how to configure defaults when building a derivative.\n“Upgrades” are not carried over the usual way of treating each single package individually: Elemental considers the container image as a new system where to boot into. It will pull a new container image during this phase, which will be booted on the next reboot.\nUpgrade to a specific container image To specify a specific container image to upgrade to instead of the regular upgrade channels, run elemental upgrade --system.uri imagei-uri.\nNote by default elemental upgrade --system.uri runs an mtree checksum verificatiom (requires images manifests generated with mtree separately). To disable image checksum verification, run elemental upgrade --verify --system.uri.\nIntegration with System Upgrade Controller If running a kubernetes cluster on the Elemental system, you can leverage the system-upgrade-controller to trigger upgrades to specific image versions, for example:\n--- apiVersion: upgrade.cattle.io/v1 kind: Plan metadata: name: elemental-upgrade namespace: system-upgrade labels: k3s-upgrade: server spec: concurrency: 1 version: fleet-sample # Image tag nodeSelector: matchExpressions: - {key: k3s.io/hostname, operator: Exists} serviceAccountName: system-upgrade cordon: true # drain: # force: true upgrade: image: quay.io/costoolkit/test-images # Image upgrade reference command: - \"/usr/sbin/suc-upgrade\" See also trigger upgrades with fleet\nFrom ISO The ISO can be also used as a recovery medium: type elemental upgrade from a LiveCD. It will then try to upgrade the image of the active partition installed in the system.\nHow it works Elemental during installation sets two .img images files in the COS_STATE partition:\n/cOS/active.img labeled COS_ACTIVE: Where Elemental typically boots from /cOS/passive.img labeled COS_PASSIVE: Where Elemental boots for fallback Those are used by the upgrade mechanism to prepare and install a pristine Elemental each time an upgrade is attempted.\n","categories":"","description":"How to run upgrades in Elemental\n","excerpt":"How to run upgrades in Elemental\n","ref":"/docs/getting-started/upgrading/","tags":"","title":"Upgrading"},{"body":" Section under construction.\nWhile building a derivative, or on a running system things can go really wrong, the guide is aimed to give tips while building derivatives and also debugging running systems.\nDon’t forget tocheck the known issues for the release you’re using.\nBefore booting, several kernel parameters can be used to help during debugging (also when booting an ISO). Those are meant to be used only while debugging, and they might defeat the concept of immutability.\nDisable Immutability By adding rd.cos.debugrw to the boot parameters read only mode will be disabled. See Immutable setup for more options.\nThe derivative will boot into RW mode, that means any change made during runtime will persist across reboots. Use this feature with caution as defeats the concept of immutability.\nrd.cos.debugrw applies only to active and passive partitions. The recovery image can’t be mutated.\nNote The changes made will persist during reboots but won’t persist across upgrades. If you need to persist changes across upgrades in runtime (for example by adding additional packages on top of the derivative image), see how to apply persistent changes. Debug initramfs issues As derivative can ship and build their own initrd, the official debug docs contains valid information that can be used for troubleshooting.\nFor example:\nrd.break=pre-mount rd.shell: Drop a shell before setting up mount points rd.break=pre-pivot rd.shell: Drop a shell before switch-root Recovery partition If you can boot into the system, the recovery partition can be used to reset the state of the active/passive, but can also be used to upgrade to specific images. Be sure to read the Recovery section in the docs.\nMutating derivative images It can be useful to mutate derivative images and commit a container’s file changes or settings into a new image. This allows you to debug a container by running an interactive shell, and re-use the mutated image in Elemental systems. Generally, it is better to use Dockerfiles to manage your images in a documented and maintainable way. Read more about creating bootable images.\nLet’s suppose we have the derivative original image at $IMAGE and we want to mutate it. We will push it later with another name $NEW_IMAGE and use it to our node downstream.\nRun the derivative image locally, and perform any necessary change (e.g. add additional software):\n$\u003e docker run --entrypoint /bin/bash -ti --name updated-image $IMAGE Commit any changes to a new image $NEW_IMAGE:\n$\u003e docker commit updated-image $NEW_IMAGE And push the image to the container registry:\n$\u003e docker push $NEW_IMAGE In the derivative then it’s sufficient to upgrade to that image with elemental upgrade:\n$\u003e elemental upgrade --docker-image $NEW_IMAGE Adding login keys at boot To add users key from the GRUB menu prompt, edit the boot cmdline and add the following kernel parameters:\nstages.boot[0].authorized_keys.root[0]=github:suse\n","categories":"","description":"Stuff can go wrong. This document tries to make them right with some useful tips\n","excerpt":"Stuff can go wrong. This document tries to make them right with some …","ref":"/docs/reference/troubleshooting/","tags":"","title":"Troubleshooting"},{"body":"","categories":"","description":"Examples and recipes for building Elemental derivatives\n","excerpt":"Examples and recipes for building Elemental derivatives\n","ref":"/docs/examples/","tags":"","title":"Examples"},{"body":"Elemental derivatives shares a common recovery mechanism built-in which can be leveraged to restore the system to a known point. At installation time, the recovery partition is created from the installation medium.\nThe recovery system can be accessed during boot by selecting the last entry in the menu (labeled by “recovery”).\nA derivative can be recovered anytime by booting into the recovery partition and by running elemental reset from it.\nThis command will regenerate the bootloader and the images in the COS_STATE partition by using the recovery image.\nUpgrading the recovery partition From either the active or passive system, the recovery partition can also be upgraded by running\nelemental upgrade --recovery It also supports to specify docker images directly:\nelemental upgrade --recovery --recovery-system.uri \u003cimage-uri\u003e Where \u003cimage-uri\u003e can be an opaque URI of docker scheme (e.g. docker:registry.org/some/image:tag).\nUpgrading the active system from the recovery The recovery system can upgrade also the active system by running elemental upgrade, and it also supports to specify docker images directly:\nelemental upgrade --system.uri \u003cimage-uri\u003e ","categories":"","description":"How to use the recovery partition to reset the system or perform upgrades.\n","excerpt":"How to use the recovery partition to reset the system or perform …","ref":"/docs/getting-started/recovery/","tags":"","title":"Recovery"},{"body":"Installing required dependencies for local build To get requirements installed locally, run:\n$\u003e make deps or you need:\nelemental-cli squashfs-tools zypper in squashfs on SLES or openSUSE xorriso zypper in xorriso on SLES or openSUSE mtools zypper in mtools on SLES or openSUSE yq (version 4.x), installed via packages/toolchain/yq (optional) jq, installed via packages/utils/jq (optional) elemental elemental comes with Elemental-toolkit\nYou can grab the binary from elemental releases.\nyq and jq yq (version 4.x) and jq are used to retrieve the list of packages to build in order to produce the final ISOs. Those are not strictly required, see the Note below.\nNote: yq and jq are just used to generate the list of packages to build, and you don’t need to have them installed if you manually specify the packages to be compiled.\n","categories":"","description":"Building prerequisites\n","excerpt":"Building prerequisites\n","ref":"/docs/development/dependencies/","tags":"","title":"Build requirements"},{"body":"Elemental vanilla images, like ISOs, cloud images or raw disks can be used to deploy another derivative image.\nelemental reset elemental reset can be used to reset the system from the recovery image or from a custom image. Vanilla images only include a minimal recovery partition and system.\nIt can be either invoked manually with elemental reset --system.uri \u003cimg-ref\u003e or used in conjuction with a cloud-init configuration, for example consider the following cloud-init configuration file that creates the state and persistent partitions during first boot (this is required on Elemental vanilla images):\nname: \"Default deployment\" stages: rootfs.after: - name: \"Repart image\" layout: # It will partition a device including the given filesystem label or part label (filesystem label matches first) device: label: COS_RECOVERY add_partitions: - fsLabel: COS_STATE # 10Gb for COS_STATE, so the disk should have at least 16Gb size: 10240 pLabel: state - fsLabel: COS_PERSISTENT # unset size or 0 size means all available space pLabel: persistent network: - if: '[ -f \"/run/cos/recovery_mode\" ]' name: \"Deploy Elemental system\" commands: - | # Use `elemental reset --system.uri docker:\u003cimg-ref\u003e` to deploy a custom image # By default the recovery Elemental gets deployed elemental reset --reboot --system.uri docker:$IMAGE The following will first repartition the image after the rootfs stage and will run elemental reset when booting into recovery mode. RAW vanilla disk images automatically boot by default into recovery, so the first thing upon booting is deploying the system\n","categories":"","description":"How to deploy derivatives images from Elemental vanilla images\n","excerpt":"How to deploy derivatives images from Elemental vanilla images\n","ref":"/docs/getting-started/deploy/","tags":"","title":"Deploying"},{"body":"Here is a not-exhaustive lists of derivatives built with Elemental.\nRun by Rancher Name Description Link Harvester Open source hyperconverged infrastructure (HCI) software https://github.com/harvester/harvester Elemental Immutable Linux distribution built to run Rancher and its corresponding Kubernetes distributions RKE2 and k3s. https://github.com/rancher/elemental Run by Community The following derivatives are not run by Rancher, but are community efforts. Open up an issue or create a PR to get yours added to the list!\nName Description Link Kairos Immutable OS for Automated (Decentralized) Kubernetes clusters with k3s, for homelab and beyond https://github.com/kairos-io/kairos ","categories":"","description":"Linux derivatives built with Elemental\n","excerpt":"Linux derivatives built with Elemental\n","ref":"/docs/reference/built_with_elemental/","tags":"","title":"Built with Elemental"},{"body":"Elemental toolkit High level Architecture This page tries to encompass the elemental-toolkit structure and the high level architecture, along with all the involved components.\nDesign goals Blueprints to build immutable Linux derivatives from container images A workflow to maintain, support and deliver custom-OS and upgrades to end systems Derivatives have the same “foundation” manifest - easy to customize on top, add packages: systemd, dracut and grub as a foundation stack. Upgrades delivered with container registry images ( also workflow with docker run \u0026\u0026 docker commit supported! ) The content of the container image is the system which is booted. High level overview elemental-toolkit encompasses several components required for building and distributing OS images. This issue summarize the current state, and how we plan to integrate them in a single CLI to improve the user experience.\nelemental-toolkit is also a manifest, which includes package definitions of how the underlying OS is composed. It forms an abstraction layer, which is then translated to Dockerfiles and built by our CI (optionally) for re-usal. A derivative can be built by parts of the manifest, or reusing it entirely, container images included.\nThe fundamental phases can be summarized in the following steps:\nBuild packages from container images (and optionally keep build caches) Extract artefacts from containers Add metadata(s) and create a repository (optionally) publish the repository and the artefacts The developer of the derivative applies a customization layer during build, which is an augmentation layer in the same form of elemental-toolkit itself.\nDistribution The OS delivery mechanism is done via container registries. The developer that wants to provide upgrades for the custom OS will push the resulting container images to the container registry. It will then be used by the installed system to pull upgrades from.\nUpgrade mechanism There are two different upgrade mechanisms available that can be used from a maintainer perspective: (a) release channels or (b) providing a container image reference ( e.g. my.registry.com/image:tag ) that can be tweaked in the customization phases to achieve the desired effect.\n","categories":"","description":"High level architecture of Elemental and its components.\n","excerpt":"High level architecture of Elemental and its components.\n","ref":"/docs/reference/high_level_architecture/","tags":"","title":"High level architecture"},{"body":"","categories":"","description":"Elemental tutorials and real life use-case samples\n","excerpt":"Elemental tutorials and real life use-case samples\n","ref":"/docs/tutorials/","tags":"","title":"Tutorials"},{"body":" Github project for our sprint board Samples repositories Build a derivative and upgrade it with fleet ","categories":"","description":"References for Elemental derivatives, like common featuresets, high level architecture\n","excerpt":"References for Elemental derivatives, like common featuresets, high …","ref":"/docs/reference/","tags":"","title":"Reference"},{"body":"Welcome!\nThe Elemental (containerized OS) distribution is entirely built over GitHub. You can check the pipelines in the .github folder to see how the process looks like.\nForking and test on your own By forking the Elemental-toolkit repository, you already have the Github Action workflow configured to start building and pushing your own Elemental fork.\nBuilding locally The elemental-cli can be built locally using go:\nFrom your git folder:\n$\u003e make build-cli $\u003e build/elemental version v0.2.5+g4d5d1be Build an example locally Building locally has a set of dependencies that should be satisfied.\nThen you can run\n# make build-os Build ISO If using SLES or openSUSE, first install the required deps:\n# zypper in -y squashfs xorriso dosfstools and then, simply run\n# make build-iso Run with qemu After you have the iso locally, run\n$\u003e make prepare-installer-test This will create a disk image and boot from the ISO.\nIf the image already exists, it will NOT be overwritten.\nYou need to run an explicit make test-clean to wipe the image and start over.\nInstalling After booting from the ISO you can log in as root with password cos using ssh ssh root@localhost:2222 and install Elemental on the disk image with:\n# elemental install /dev/sda Run tests Requires: ginkgo, qemu\nWe have a test suite which runs over SSH.\nTo create the disk image:\n$\u003e make build-disk To run the tests:\n$\u003e make test-smoke ","categories":"","description":"How to build Elemental?\n","excerpt":"How to build Elemental?\n","ref":"/docs/development/","tags":"","title":"Development"},{"body":"What is Elemental toolkit? Elemental is a toolkit which allows container images to be bootable in VMs, baremetals, embedded devices, and much more.\nElemental allows to create meta-Linux derivatives which are configured throughout cloud-init configuration files and are immutable by default.\nElemental and derivatives shares a common feature set, can be upgraded with a A/B mechanism, and upgrades are delivered with standard container registries.\nElemental comes also with vanilla images that can be used to boot directly container images built with the toolkit.\nWhy Elemental? Elemental allows to create custom OS versions in your cluster with standard container images with a high degree of customization. It can also be used in its vanilla form - Elemental enables then everyone to build their own derivative and access it in various formats.\nBuilding a bootable image is as simple as running docker build.\nWhat is it good for?: Embedded, Cloud, Containers, VM, Baremetals, Servers, IoT, Edge Design goals A Manifest for container-based OS. It contains just the common bits to make a container image bootable and to be upgraded from, with few customization on top Everything is an OCI artifact from the ground-up Immutable-first, but with a flexible layout Cloud-init driven Based on systemd Built and upgraded from containers - It is a single image OS! A/B updates Easy to customize Cryptographically verified Instant switch from different versions Recovery mechanism with Elemental vanilla images (or bring your own) Mission The elemental-toolkit project is under the Elemental umbrella.\nElemental-toolkit provides a unique container based approach to define the system lifecycle of an immutable Linux derivative, without any string attached to a specific Linux distribution.\nAt its heart, Elemental-toolkit is the abstraction layer between Linux distro management and the specific purpose of the OS.\nElemental-toolkit empowers anyone to create derivatives from standard OCI images. Frees whoever wants to create a Linux derivative from handling the heavy bits of packaging and managing entire repositories to propagate upgrades, simplifying the entire process by using container images as base for OS. At the same time, Elemental-toolkit provides an highly integrated ecosystem which is designed to be container-first, cloud native, and immutable. Anyone can tweak Elemental-toolkit derivatives from the bottom-up to enable and disable its featureset.\nAs the Elemental team, the elemental project is our point of reference.\nElemental is a complete derivative built with elemental-toolkit tied with the rancher ecosystem and full cycle node management solution with Kubernetes.\nWe are supporting directly and indirectly elemental within changes also in the Elemental ecosystem.\nElemental is our main show-case, and as the Elemental team we are committed to it. It encompasses several technologies to create a Kubernetes-focused Linux derivative which lifecycle is managed entirely from Kubernetes itself, Secure Device Onboarding included, and automatic provisioning via cloud-init.\n","categories":"","description":"","excerpt":"What is Elemental toolkit? Elemental is a toolkit which allows …","ref":"/docs/","tags":"","title":"Documentation"},{"body":" Immutable Linux Derivatives at your fingertips Elemental is a toolkit to build, ship and maintain cloud-init driven Linux derivatives with containers.. Learn More Great For Embedded Appliances Edge True DevOps Why Use Elemental toolkit ? Built for DevOps Elemental toolkit allows to maintain custom Linux derivatives in a GitOps style with container registries.\nOperational Happiness Upgrade machines from container images in OTA style - manually or automatically within kubernetes.\nBring your own OS A common featureset shared between Elemental derivatives, completely pluggable.\nSingle Image OS derivatives built with Elemental are single container image OS that are bootable from a system. They follow an A/B update style and gets upgrades from regular container registries.\nHow it Works\nGet Started Step 1.git clone https://github.com/rancher/elemental-toolkit Step 2.docker build -t example examples/standard Step 3.elemental upgrade example That’s it! With just a few commands, you have your own custom OS.\nLearn More Ready to upgrade?\nBuild with Elemental toolkit today ","categories":"","description":"Immutable Linux Derivatives at your fingertips","excerpt":"Immutable Linux Derivatives at your fingertips","ref":"/","tags":"","title":"Elemental toolkit"},{"body":"","categories":"","description":"","excerpt":"","ref":"/search/","tags":"","title":"Search Results"}]